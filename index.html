<!DOCTYPE html>
<!--
AI HSR Risk Reference Tool v2.0
Copyright (C) 2025 Tamiko Eto, TechInHSR

This work is dual-licensed:
- Code (HTML/CSS/JavaScript): GNU Affero General Public License v3.0 (AGPL-3.0)
- Content & Documentation: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0)

You are free to use, share, and modify this tool for NON-COMMERCIAL purposes only.
See LICENSE.md for complete terms.

For commercial licensing inquiries: tamiko@techinhsr.com
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI HSR Risk Reference Tool v2.0</title>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #1a5490;
            --primary-dark: #0f3d6b;
            --accent: #e67e22;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
            --text: #2c3e50;
            --text-light: #7f8c8d;
            --bg: #f8f9fa;
            --white: #ffffff;
            --border: #dee2e6;
            --shadow: rgba(0, 0, 0, 0.08);
        }

        body {
            font-family: 'IBM Plex Sans', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
        }

        .header {
            background: linear-gradient(135deg, var(--primary) 0%, var(--primary-dark) 100%);
            color: var(--white);
            padding: 2rem 1rem;
            box-shadow: 0 4px 12px var(--shadow);
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 1rem;
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .version {
            display: inline-block;
            background: rgba(255, 255, 255, 0.2);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.875rem;
            font-weight: 500;
            margin-left: 1rem;
        }

        .subtitle {
            font-size: 1rem;
            opacity: 0.9;
            font-weight: 400;
        }

        .author {
            text-align: right;
            font-size: 0.875rem;
            opacity: 0.9;
        }

        .nav {
            background: var(--white);
            border-bottom: 2px solid var(--border);
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 8px var(--shadow);
        }

        .nav-content {
            display: flex;
            gap: 0.5rem;
            padding: 0.75rem 0;
            overflow-x: auto;
        }

        .nav-btn {
            background: var(--white);
            border: 2px solid var(--border);
            padding: 0.5rem 1.25rem;
            border-radius: 8px;
            cursor: pointer;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s;
            white-space: nowrap;
            font-family: inherit;
        }

        .nav-btn:hover {
            border-color: var(--primary);
            background: rgba(26, 84, 144, 0.05);
        }

        .nav-btn.active {
            background: var(--primary);
            color: var(--white);
            border-color: var(--primary);
        }

        .main {
            padding: 2rem 0;
        }

        .section {
            display: none;
            animation: fadeIn 0.3s ease;
        }

        .section.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .card {
            background: var(--white);
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 2px 8px var(--shadow);
            border: 1px solid var(--border);
        }

        .card h2 {
            color: var(--primary);
            margin-bottom: 1rem;
            font-size: 1.5rem;
            font-weight: 600;
        }

        .card h3 {
            color: var(--text);
            margin: 1.5rem 0 0.75rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        .filters {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .filter-group label {
            font-weight: 600;
            font-size: 0.875rem;
            color: var(--text);
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        select {
            padding: 0.75rem;
            border: 2px solid var(--border);
            border-radius: 8px;
            font-size: 1rem;
            font-family: inherit;
            background: var(--white);
            cursor: pointer;
            transition: all 0.2s;
        }

        select:hover, select:focus {
            border-color: var(--primary);
            outline: none;
        }

        .risk-card {
            background: var(--white);
            border: 2px solid var(--border);
            border-left: 4px solid var(--primary);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1rem;
            transition: all 0.2s;
        }

        .risk-card:hover {
            box-shadow: 0 4px 12px var(--shadow);
            transform: translateY(-2px);
        }

        .risk-title {
            font-size: 1.125rem;
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 0.75rem;
        }

        .risk-meta {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
        }

        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .badge-phase1 { background: #e3f2fd; color: #1976d2; }
        .badge-phase2 { background: #fff3e0; color: #f57c00; }
        .badge-phase3 { background: #fce4ec; color: #c2185b; }
        .badge-domain { background: #f3e5f5; color: #7b1fa2; }

        .risk-description {
            color: var(--text);
            margin-bottom: 1rem;
            line-height: 1.7;
        }

        .mitigation-section, .reviewer-section {
            background: var(--bg);
            border-radius: 6px;
            padding: 1rem;
            margin-top: 1rem;
        }

        .section-title {
            font-weight: 600;
            color: var(--text);
            margin-bottom: 0.5rem;
            font-size: 0.875rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .mitigation-section { border-left: 3px solid var(--success); }
        .reviewer-section { border-left: 3px solid var(--accent); }

        ul {
            margin-left: 1.5rem;
            margin-top: 0.5rem;
        }

        li {
            margin-bottom: 0.5rem;
            color: var(--text);
        }

        .info-box {
            background: #e8f4f8;
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            border-radius: 6px;
            margin: 1rem 0;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 4px solid var(--warning);
            padding: 1rem 1.5rem;
            border-radius: 6px;
            margin: 1rem 0;
        }

        .phase-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .phase-card {
            background: var(--white);
            border: 2px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            transition: all 0.3s;
        }

        .phase-card:hover {
            border-color: var(--primary);
            box-shadow: 0 4px 12px var(--shadow);
        }

        .phase-number {
            display: inline-block;
            width: 40px;
            height: 40px;
            background: var(--primary);
            color: var(--white);
            border-radius: 50%;
            text-align: center;
            line-height: 40px;
            font-weight: 700;
            font-size: 1.25rem;
            margin-bottom: 1rem;
        }

        .domain-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1rem;
            margin-top: 1rem;
        }

        .domain-card {
            background: linear-gradient(135deg, var(--white) 0%, var(--bg) 100%);
            border: 2px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            transition: all 0.2s;
        }

        .domain-card:hover {
            border-color: var(--primary);
            transform: translateY(-2px);
        }

        .domain-icon {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        .no-results {
            text-align: center;
            padding: 3rem;
            color: var(--text-light);
        }

        .footer {
            background: var(--text);
            color: var(--white);
            padding: 2rem 0;
            margin-top: 3rem;
            text-align: center;
        }

        .footer a {
            color: var(--accent);
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        code {
            font-family: 'IBM Plex Mono', monospace;
            background: var(--bg);
            padding: 0.125rem 0.375rem;
            border-radius: 3px;
            font-size: 0.875em;
        }

        @media (max-width: 768px) {
            h1 { font-size: 1.5rem; }
            .header-content { flex-direction: column; align-items: flex-start; }
            .author { text-align: left; }
            .filters { grid-template-columns: 1fr; }
            .phase-grid, .domain-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>AI HSR Risk Reference Tool<span class="version">v2.0</span></h1>
                    <p class="subtitle">Quick Reference Risk Identification and Mitigation Guide for IRBs Reviewing AI in Human Subjects Research</p>
                </div>
                <div class="author">
                    <strong>Tamiko Eto, MA CIP</strong><br>
                    Founder: TechInHSR
                </div>
            </div>
        </div>
    </header>

    <nav class="nav">
        <div class="container">
            <div class="nav-content">
                <button class="nav-btn active" onclick="showSection('overview')">Overview</button>
                <button class="nav-btn" onclick="showSection('framework')">3-Phase Framework</button>
                <button class="nav-btn" onclick="showSection('domains')">Risk Domains</button>
                <button class="nav-btn" onclick="showSection('tool')">Interactive Tool</button>
                <button class="nav-btn" onclick="showSection('prompts')">Reviewer Prompts</button>
                <button class="nav-btn" onclick="showSection('definitions')">Definitions</button>
                <button class="nav-btn" onclick="showSection('about')">About</button>
            </div>
        </div>
    </nav>

    <main class="main container">
        <!-- Overview Section -->
        <section id="overview" class="section active">
            <div class="card">
                <h2>Welcome to the AI HSR Risk Reference Tool</h2>
                <p>This tool assists Institutional Review Boards (IRBs) and Ethics Committees in identifying and addressing AI-specific risks in human subjects research. It complements standard IRB review processes by providing structured guidance on risks unique to artificial intelligence and machine learning systems.</p>
                
                <div class="warning-box">
                    <strong>Important:</strong> This tool focuses on AI-specific risks and does not replace comprehensive IRB review. General research risks (privacy, physical/psychological harm, deception) should be addressed through standard IRB processes.
                </div>

                <h3>Key Features</h3>
                <ul>
                    <li><strong>Structured Framework:</strong> Organized around the 3-Phase AI HSR IRB Review Framework</li>
                    <li><strong>Evidence-Based:</strong> Built on MIT AI Risk Repository, ISO 14971, and U.S. regulatory frameworks</li>
                    <li><strong>Practical Guidance:</strong> Includes reviewer prompts and mitigation strategies</li>
                    <li><strong>Validated:</strong> Tested at 23+ institutions with 21% improvement in reviewer confidence</li>
                </ul>

                <h3>Four Core AI-Specific Risks</h3>
                <div class="domain-grid">
                    <div class="domain-card">
                        <div class="domain-icon">‚öñÔ∏è</div>
                        <h4>Misclassification</h4>
                        <p>Incorrect categorization of participants, diagnoses, or outcomes that can lead to inappropriate interventions.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üîç</div>
                        <h4>Explainability</h4>
                        <p>Opacity of AI models where neither researchers nor participants fully understand how predictions are made.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üë•</div>
                        <h4>Participant Vulnerability & Equity</h4>
                        <p>Uneven AI performance across demographic groups that may exacerbate health disparities.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üîí</div>
                        <h4>Data Sensitivity & Privacy</h4>
                        <p>Concerns about confidentiality, secondary use, reidentifiability, and HIPAA compliance with large datasets.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Framework Section -->
        <section id="framework" class="section">
            <div class="card">
                <h2>3-Phase AI HSR IRB Review Framework</h2>
                <p>The framework aligns AI research oversight with project maturity to avoid over- and under-regulation:</p>

                <div class="phase-grid">
                    <div class="phase-card">
                        <div class="phase-number">1</div>
                        <h3>Discover/Ideation</h3>
                        <p><strong>Focus:</strong> Early exploratory work</p>
                        <p><strong>Activities:</strong> Data collection, preliminary analysis, proof of concept</p>
                        <p><strong>Risk Level:</strong> Lower - limited participant interaction</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Data quality and representativeness</li>
                                <li>Initial bias assessment</li>
                                <li>Privacy protections for training data</li>
                            </ul>
                        </div>
                    </div>

                    <div class="phase-card">
                        <div class="phase-number">2</div>
                        <h3>Pilot/Validation</h3>
                        <p><strong>Focus:</strong> Model performance testing</p>
                        <p><strong>Activities:</strong> Validation studies, algorithm testing, performance metrics</p>
                        <p><strong>Risk Level:</strong> Medium - controlled testing environment</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Model explainability requirements</li>
                                <li>Performance across subgroups</li>
                                <li>Error handling and safety mechanisms</li>
                            </ul>
                        </div>
                    </div>

                    <div class="phase-card">
                        <div class="phase-number">3</div>
                        <h3>Clinical Investigation / Real-World Deployment</h3>
                        <p><strong>Focus:</strong> Real-world use and impact</p>
                        <p><strong>Activities:</strong> Clinical trials, deployment studies, post-market surveillance</p>
                        <p><strong>Risk Level:</strong> Higher - direct impact on care decisions</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Clinical decision-making integration</li>
                                <li>Monitoring and adverse event reporting</li>
                                <li>Long-term equity impacts</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Risk Domains Section -->
        <section id="domains" class="section">
            <div class="card">
                <h2>MIT AI Risk Domains</h2>
                <p>This tool focuses on four of MIT's seven major AI risk domains most relevant to human subjects research:</p>

                <h3>1. Discrimination and Toxicity</h3>
                <p>Concerns about biased or harmful outputs where AI systems may perpetuate unfair treatment or expose participants to inappropriate content.</p>
                <ul>
                    <li>Algorithmic bias across demographic groups</li>
                    <li>Discriminatory predictions or recommendations</li>
                    <li>Toxic or offensive outputs in generative systems</li>
                    <li>Perpetuation of stereotypes</li>
                </ul>

                <h3>2. Privacy and Security</h3>
                <p>Protecting sensitive research data and ensuring systems are resilient to breaches, leaks, and unauthorized use.</p>
                <ul>
                    <li>Data confidentiality and de-identification</li>
                    <li>Risk of re-identification</li>
                    <li>Unauthorized access or data breaches</li>
                    <li>HIPAA and Privacy Rule compliance</li>
                    <li>Model inversion attacks</li>
                </ul>

                <h3>3. Misinformation</h3>
                <p>Risk of false outputs or hallucinations that can mislead researchers and participants if left unchecked.</p>
                <ul>
                    <li>AI hallucinations (fabricated information)</li>
                    <li>Incorrect clinical recommendations</li>
                    <li>Misleading data summaries</li>
                    <li>Confidence in incorrect predictions</li>
                </ul>

                <h3>4. Human-Computer Interaction</h3>
                <p>Preserving human judgment in research and clinical application, ensuring that humans remain the ultimate decision-makers.</p>
                <ul>
                    <li>Over-reliance on AI recommendations</li>
                    <li>Automation bias in clinical decisions</li>
                    <li>Informed consent challenges</li>
                    <li>User interface design and clarity</li>
                    <li>Appropriate human oversight mechanisms</li>
                </ul>
            </div>
        </section>

        <!-- Interactive Tool Section -->
        <section id="tool" class="section">
            <div class="card">
                <h2>Interactive Risk Assessment</h2>
                <p>Select filters below to view relevant risks, mitigation strategies, and reviewer prompts:</p>

                <div class="filters">
                    <div class="filter-group">
                        <label for="phaseFilter">Development Phase</label>
                        <select id="phaseFilter" onchange="filterRisks()">
                            <option value="all">All Phases</option>
                            <option value="phase1">Phase 1: Discover/Ideation</option>
                            <option value="phase2">Phase 2: Pilot/Validation</option>
                            <option value="phase3">Phase 3: Clinical Investigation</option>
                        </select>
                    </div>

                    <div class="filter-group">
                        <label for="domainFilter">Risk Domain</label>
                        <select id="domainFilter" onchange="filterRisks()">
                            <option value="all">All Domains</option>
                            <option value="discrimination">Discrimination & Toxicity</option>
                            <option value="privacy">Privacy & Security</option>
                            <option value="misinformation">Misinformation</option>
                            <option value="hci">Human-Computer Interaction</option>
                        </select>
                    </div>

                    <div class="filter-group">
                        <label for="riskFilter">Specific Risk</label>
                        <select id="riskFilter" onchange="filterRisks()">
                            <option value="all">All Risks</option>
                            <option value="misclassification">Misclassification</option>
                            <option value="explainability">Explainability</option>
                            <option value="equity">Participant Vulnerability & Equity</option>
                            <option value="data-privacy">Data Sensitivity & Privacy</option>
                        </select>
                    </div>
                </div>

                <div id="risksContainer"></div>
            </div>
        </section>

        <!-- Reviewer Prompts Section -->
        <section id="prompts" class="section">
            <div class="card">
                <h2>üìã Reviewer Prompts for IRBs</h2>
                <p><strong>Purpose:</strong> Use these prompts to communicate with research teams about specific risks identified in their AI protocols. Select the development phase and risk domain to see relevant prompts.</p>

                <div class="info-box">
                    <strong>How to Use:</strong> Filter by phase and domain, then copy the relevant prompt(s) to include in your IRB correspondence, stipulation letters, or review notes.
                </div>

                <div class="filters">
                    <div class="filter-group">
                        <label for="promptPhaseFilter">Development Phase</label>
                        <select id="promptPhaseFilter" onchange="filterPrompts()">
                            <option value="all">All Phases</option>
                            <option value="phase1">Phase 1: Discover/Ideation</option>
                            <option value="phase2">Phase 2: Pilot/Validation</option>
                            <option value="phase3">Phase 3: Clinical Investigation</option>
                        </select>
                    </div>

                    <div class="filter-group">
                        <label for="promptDomainFilter">Risk Domain</label>
                        <select id="promptDomainFilter" onchange="filterPrompts()">
                            <option value="all">All Domains</option>
                            <option value="discrimination">Discrimination & Toxicity</option>
                            <option value="privacy">Privacy & Security</option>
                            <option value="misinformation">Misinformation</option>
                            <option value="malicious">Malicious Actors & Misuse</option>
                            <option value="hci">Human-Computer Interaction</option>
                            <option value="socioeconomic">Socioeconomic & Environmental</option>
                            <option value="system-safety">AI System Safety & Limitations</option>
                        </select>
                    </div>
                </div>

                <div id="promptsContainer"></div>
            </div>
        </section>

        <!-- Definitions Section -->
        <section id="definitions" class="section">
            <div class="card">
                <h2>Key Definitions</h2>

                <h3>AI Human Subjects Research (AI HSR)</h3>
                <p>AI human subjects research is "Research" involving "human subjects", conducted to develop AI tools.</p>

                <h3>Common AI Model Types</h3>
                <ul>
                    <li><strong>Predictive Models:</strong> Systems that forecast outcomes based on historical data (e.g., risk calculators, diagnostic algorithms)</li>
                    <li><strong>Large Language Models (LLMs):</strong> AI systems trained on vast text data to understand and generate human language</li>
                    <li><strong>Foundation Models:</strong> Large-scale models trained on broad data that can be adapted for multiple tasks</li>
                    <li><strong>Generative AI:</strong> Systems that create new content (text, images, code) based on learned patterns</li>
                    <li><strong>Classification Models:</strong> Algorithms that categorize data into predefined groups</li>
                    <li><strong>Computer Vision:</strong> AI systems that interpret and analyze visual information</li>
                </ul>

                <h3>Key Regulatory Frameworks</h3>
                <ul>
                    <li><strong>45 CFR 46 (Common Rule):</strong> Federal policy for protection of human research subjects</li>
                    <li><strong>21 CFR 56:</strong> FDA regulations for IRB oversight</li>
                    <li><strong>21 CFR 812:</strong> Investigational Device Exemptions (IDE)</li>
                    <li><strong>HIPAA Privacy Rule:</strong> Standards for protecting health information</li>
                    <li><strong>ISO 14971:</strong> International standard for risk management in medical devices</li>
                </ul>

                <h3>Belmont Principles Applied to AI</h3>
                <ul>
                    <li><strong>Respect for Persons:</strong> Informed consent about AI use, explainability requirements</li>
                    <li><strong>Beneficence:</strong> Maximizing benefits and minimizing harms from AI systems</li>
                    <li><strong>Justice:</strong> Fair distribution of AI benefits and burdens across populations</li>
                </ul>
            </div>
        </section>

        <!-- About Section -->
        <section id="about" class="section">
            <div class="card">
                <h2>About This Tool</h2>
                
                <h3>Development & Validation</h3>
                <p>The AI HSR Risk Reference Tool was developed through a structured, iterative design process as part of a safety engineering project at the Center for AI Safety (CAIS). The tool has been:</p>
                <ul>
                    <li>Validated at 23+ institutions nationally</li>
                    <li>Shown to improve reviewer confidence by 21%</li>
                    <li>Demonstrated 60% reduction in revision cycles</li>
                    <li>Adopted by institutions including University of Washington and Multi-Regional Clinical Trials Center</li>
                </ul>

                <h3>Methodology</h3>
                <p>The tool maps risks and safeguards from the MIT AI Risk Library and MIT AI Risk Mitigation Library against:</p>
                <ul>
                    <li>ISO 14971 (risk management for medical devices)</li>
                    <li>45 CFR 46 (Common Rule)</li>
                    <li>21 CFR Parts 312, 812, and 820 (FDA regulations)</li>
                    <li>HIPAA Privacy Rule</li>
                    <li>Belmont Principles and Good Clinical Practice (GCP)</li>
                </ul>

                <h3>Scope & Limitations</h3>
                <p><strong>Current Version Includes:</strong></p>
                <ul>
                    <li>AI-specific risks under U.S. human subjects regulations (45 CFR 46)</li>
                    <li>Focus on complex AI systems (predictive models, LLMs, foundation models)</li>
                    <li>Four core risk domains relevant to HSR</li>
                </ul>

                <p><strong>Future Versions Will Include:</strong></p>
                <ul>
                    <li>International regulations (EU AI Act, GDPR)</li>
                    <li>ISO standards (42001, 23894, 42005, 24368)</li>
                    <li>Patient and community perspectives</li>
                    <li>Integration with IRB electronic platforms</li>
                </ul>

                <h3>How to Use This Tool</h3>
                <div class="info-box">
                    <ol>
                        <li>Navigate to the <strong>Interactive Tool</strong> section</li>
                        <li>Select the development phase of the AI system under review</li>
                        <li>Choose relevant risk domains</li>
                        <li>Review identified risks, mitigation strategies, and reviewer prompts</li>
                        <li>Use prompts to guide IRB deliberations</li>
                        <li>Document findings in your IRB review materials</li>
                    </ol>
                </div>

                <h3>Citation</h3>
                <p>If you use this tool in your work, please cite:</p>
                <p style="font-style: italic; padding-left: 2rem;">
                    Eto, T. (2025). AI HSR Risk Reference Tool v2.0: Quick Reference Risk Identification and Mitigation Guide for IRBs Reviewing AI in Human Subjects Research. TechInHSR.
                </p>

                <h3>Resources</h3>
                <ul>
                    <li><a href="https://techinhsr.com" target="_blank">TechInHSR.com</a> - Blog and updates</li>
                    <li><a href="https://github.com/etotamikolab/TechInHSR" target="_blank">GitHub Repository</a> - Access the Excel version</li>
                    <li><a href="https://ai-hsr-risk-reference.glide.page" target="_blank">Glide App v1.5</a> - Previous web version</li>
                </ul>

                <h3>Acknowledgements</h3>
                <p>Special thanks to Professor Josep Curto, PhD, at the Center for AI Safety (CAIS) for invaluable guidance, and to colleagues Mark Lifson, Heather Miller, and the broader IRB community for their feedback and support.</p>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Tamiko Eto, MA CIP | TechInHSR</p>
            <p style="margin-top: 0.5rem;">
                <strong>Licensed under:</strong> 
                <a href="https://www.gnu.org/licenses/agpl-3.0.en.html" target="_blank">AGPL-3.0</a> (code) & 
                <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> (content)
                <br>
                <span style="font-size: 0.875rem; opacity: 0.9;">Free for non-commercial use | Commercial licensing: tamiko@techinhsr.com</span>
            </p>
            <p style="margin-top: 0.5rem;">
                <a href="mailto:tamiko@techinhsr.com">Contact</a> | 
                <a href="https://techinhsr.com" target="_blank">Website</a> | 
                <a href="https://github.com/[your-username]/ai-hsr-risk-tool" target="_blank">GitHub</a>
            </p>
        </div>
    </footer>

    <script>
        // Risk data structure
        const risks = [
        {
                "id": 1,
                "title": "Predictive triage AI",
                "phases": [
                        "phase1"
                ],
                "domains": [
                        "discrimination",
                        "privacy"
                ],
                "specificRisk": "data-privacy",
                "description": "Privacy breach, non-clinically validated AI output/data entry into medical record, immediate or future direct harm caused by inappropriate patient care/treatment with non-clinically validated AI output ",
                "model_type": "Predictive Models",
                "research_features": "Uses historical EHR data; no clinical impact",
                "mitigations": [
                        "Biased results or misrepresentation: This project involves training a predictive model using de-identifiable retrospective EHR data. To mitigate risks of biased outputs, we will stratify training data by race, gender, and age and monitor for subgroup performance disparities. The model will not be deployed or used to inform patient care decisions. Perform bias analysis using stratified performance metrics across racial, gender, and age groups using retrospective datasets."
                ],
                "reviewerPrompts": {
                        "phase1": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations"
                }
        },
        {
                "id": 2,
                "title": "LLM Chat Consent Assistant",
                "phases": [
                        "phase1"
                ],
                "domains": [
                        "discrimination",
                        "privacy"
                ],
                "specificRisk": "explainability",
                "description": "Inadequate informed consent (Participants may not understand Ai-generated explanations)",
                "model_type": "Large Language Models (LLMs)",
                "research_features": "LLM outputs vary; difficult to validate accuracy",
                "mitigations": [
                        "Biased results or misrepresentation: This project involves training a predictive model using de-identifiable retrospective EHR data. To mitigate risks of biased outputs, we will stratify training data by race, gender, and age and monitor for subgroup performance disparities. The model will not be deployed or used to inform patient care decisions. Perform bias analysis using stratified performance metrics across racial, gender, and age groups using retrospective datasets."
                ],
                "reviewerPrompts": {
                        "phase1": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations"
                }
        },
        {
                "id": 3,
                "title": "LLM Chatbot providing post-discharge instructions",
                "phases": [
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy"
                ],
                "specificRisk": "explainability",
                "description": "Miscommunication; Therapeutic misconception (Participants may misunderstand chatbot suggestions as clinical orders)",
                "model_type": "Large Language Models (LLMs)",
                "research_features": "Chat-based interaction; post-care communication",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds."
                ],
                "reviewerPrompts": {
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n"
                }
        },
        {
                "id": 4,
                "title": "Predictive model for sepsis risk from EHR data",
                "phases": [
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation"
                ],
                "specificRisk": "equity",
                "description": "Biased outcomes; Safety and fairness (The model may underpredict sepsis risk in underrepresented populations.)",
                "model_type": "Supervised Machine Learning, Predictive Models",
                "research_features": "EHR-based retrospective model; potential clinical impact",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds."
                ],
                "reviewerPrompts": {
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n"
                }
        },
        {
                "id": 5,
                "title": "AI-assisted imaging for tumor boundary detection",
                "phases": [
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation"
                ],
                "specificRisk": "equity",
                "description": "Downstream care decisions may be biased by automation (Overreliance on AI imaging may affect clinician decision-making workflows)",
                "model_type": "Supervised Machine Learning",
                "research_features": "Clinical workflow augmentation; prospective study",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds."
                ],
                "reviewerPrompts": {
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n"
                }
        },
        {
                "id": 6,
                "title": "LLM summarizing pediatric patient notes for clinical trials eligibility",
                "phases": [
                        "phase3"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci",
                        "malicious",
                        "socioeconomic",
                        "system-safety"
                ],
                "specificRisk": "misclassification",
                "description": "Risk of inappropriate enrollment based on inaccurate data (The LLM may fabricate patient details, leading to inappropriate trial inclusion.)",
                "model_type": "Large Language Models (LLMs)",
                "research_features": "Automated summarization; eligibility screening",
                "mitigations": [
                        "Biased results or misrepresentation: This AI tool will be integrated into clinical workflows to provide decision support. Clinicians will be trained to review outputs critically and document whether the AI recommendation was accepted, overridden, or flagged. A staged rollout with post-deployment monitoring, user recourse mechanisms, and incident logging will be implemented. System performance, safety incidents, and subgroup disparities will be reported to the IRB quarterly. Output will be labeled as research-use only. A DSMB will review demographic impacts monthly, and clinical teams will receive training on limits of generalizability."
                ],
                "reviewerPrompts": {
                        "phase3": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups"
                }
        },
        {
                "id": 7,
                "title": "Computer vision tool for diabetic retinopathy detection",
                "phases": [
                        "phase3"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci",
                        "malicious",
                        "socioeconomic",
                        "system-safety"
                ],
                "specificRisk": "general",
                "description": "Missed diagnosis; inequitable care (Model trained on non-representative dataset may miss early signs in some populations.)",
                "model_type": "Computer Vision Models",
                "research_features": "Image classification; diagnostic support",
                "mitigations": [
                        "Biased results or misrepresentation: This AI tool will be integrated into clinical workflows to provide decision support. Clinicians will be trained to review outputs critically and document whether the AI recommendation was accepted, overridden, or flagged. A staged rollout with post-deployment monitoring, user recourse mechanisms, and incident logging will be implemented. System performance, safety incidents, and subgroup disparities will be reported to the IRB quarterly. Output will be labeled as research-use only. A DSMB will review demographic impacts monthly, and clinical teams will receive training on limits of generalizability."
                ],
                "reviewerPrompts": {
                        "phase3": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups"
                }
        },
        {
                "id": 8,
                "title": "Minority Population-Focused Behavioral nudges based on AI-identified patterns in EHR",
                "phases": [
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci",
                        "malicious",
                        "system-safety"
                ],
                "specificRisk": "explainability",
                "description": "Autonomy and transparency (AI-generated nudges may influence provider behavior without awareness or consent.)",
                "model_type": "Predictive Models, Supervised Machine Learning, Recommendation Systems, Generative Models (non-LLM)",
                "research_features": "Behavioral intervention; AI-targeted messaging",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds."
                ],
                "reviewerPrompts": {
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n"
                }
        },
        {
                "id": 9,
                "title": "Unsupervised clustering to identify mental health risk profiles in older adults",
                "phases": [
                        "phase1",
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci",
                        "malicious",
                        "system-safety"
                ],
                "specificRisk": "data-privacy",
                "description": "Privacy; stigmatization (Unique combinations of behavioral traits may lead to participant re-identification.)",
                "model_type": "Unsupervised Machine Learning, Classification Models",
                "research_features": "Behavioral data; privacy-sensitive clustering",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds.",
                        "Biased results or misrepresentation: This project involves training a predictive model using de-identifiable retrospective EHR data. To mitigate risks of biased outputs, we will stratify training data by race, gender, and age and monitor for subgroup performance disparities. The model will not be deployed or used to inform patient care decisions. Perform bias analysis using stratified performance metrics across racial, gender, and age groups using retrospective datasets."
                ],
                "reviewerPrompts": {
                        "phase1": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations",
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n"
                }
        },
        {
                "id": 10,
                "title": "Evaluation of LLM for radiology report classification to monitor performance, value and user engagement of FDA-approved AI algorithms currently deployed in radiology dept.",
                "phases": [
                        "phase1"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci"
                ],
                "specificRisk": "equity",
                "description": "Privacy/confidentiality/data handline details; waiver criteria met? Bias due to LLM performance varying by pt demographic or report structure; No effect on pt care but monitoring results could indirectly inform clinical practice over time",
                "model_type": "Predictive Models, Large Language Models (LLMs)",
                "research_features": "Retrospective chart review of radiology reports; FDA-cleared imaging algorithms already in clinical use; LLM output compared against existing AI algorithm outputs for QI",
                "mitigations": [
                        "Biased results or misrepresentation: This project involves training a predictive model using de-identifiable retrospective EHR data. To mitigate risks of biased outputs, we will stratify training data by race, gender, and age and monitor for subgroup performance disparities. The model will not be deployed or used to inform patient care decisions. Perform bias analysis using stratified performance metrics across racial, gender, and age groups using retrospective datasets."
                ],
                "reviewerPrompts": {
                        "phase1": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations"
                }
        },
        {
                "id": 11,
                "title": "Automated transcription and structuring of nuser-patient convo into EHR fields",
                "phases": [
                        "phase2"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci"
                ],
                "specificRisk": "general",
                "description": "",
                "model_type": "Large Language Models (LLMs)",
                "research_features": "",
                "mitigations": [
                        "Privacy or confidentiality breach: Data ingestion pipelines will be monitored continuously to detect inadvertent re-identification risks. Security teams will conduct membership inference and red-team testing exercises to attempt re-identification from outputs, with findings logged and reviewed by the oversight committee. Audit logs will be maintained to track any detected sensitive information leakage, and all incidents will be reported to the IRB within predetermined timelines."
                ],
                "reviewerPrompts": {
                        "phase1": "What safeguards are in place for ?",
                        "phase2": "How will the model be validated to address ?",
                        "phase3": "What monitoring will occur post-deployment for ?"
                }
        },
        {
                "id": 12,
                "title": "The AI system will record voices of clinician-patient conversations and automatically fill in discrete fields in the EHR.",
                "phases": [
                        "phase1"
                ],
                "domains": [
                        "privacy",
                        "misinformation"
                ],
                "specificRisk": "general",
                "description": "Patients may not realize conversations are being recorded; what is being done to prevent risk of accidental disclosure? How might the recordings be saved, used, and disclosed in the future and for what purpose? Will the tool \u201cremember\u201d the participant? Can end users such as sponsors/vendors/manufacturers obtain PHI from the tool directly even if PHI has been removed? Were non-English-Speaking and/or minorities excluded from training so that the tool may not perform well on these types of populations?",
                "model_type": "Predictive Models",
                "research_features": "Real-time audio capture (fully identifiable), prospective data collection, inpatient adults at least 18 years of age, clinician-verified output.",
                "mitigations": [
                        "Privacy or confidentiality breach: All training datasets will be de-identified in compliance with HIPAA Safe Harbor standards, with direct identifiers removed and privacy-enhancing techniques such as differential privacy and k-anonymity applied. Data will be stored on secure, access-controlled servers, and detailed data handling procedures will be documented in the IRB protocol. This ensures that the training process minimizes the possibility of re-identification from the outset."
                ],
                "reviewerPrompts": {
                        "phase1": "Privacy or Confidentiality Breach:\nPlease describe your process of de-identification to reduce re-identification risk"
                }
        },
        {
                "id": 13,
                "title": "Chatbot that summarizes medical records in prep for a doctor appointment",
                "phases": [
                        "phase2",
                        "phase3"
                ],
                "domains": [
                        "discrimination",
                        "privacy",
                        "misinformation",
                        "hci"
                ],
                "specificRisk": "general",
                "description": "",
                "model_type": "",
                "research_features": "",
                "mitigations": [
                        "Biased results or misrepresentation: We will validate the AI model on prospectively collected data. During this stage, clinicians will review AI-generated outputs alongside their standard workflows but will not rely on them for clinical decisions. A formal testing and auditing protocol has been established to assess subgroup accuracy, data leakage risk, and overfitting. Any potential harms or errors will be logged and reviewed weekly by the study team. No data will be entered into the medical record. Prospective validation will include subgroup performance audits with fairness thresholds.",
                        "Biased results or misrepresentation: This AI tool will be integrated into clinical workflows to provide decision support. Clinicians will be trained to review outputs critically and document whether the AI recommendation was accepted, overridden, or flagged. A staged rollout with post-deployment monitoring, user recourse mechanisms, and incident logging will be implemented. System performance, safety incidents, and subgroup disparities will be reported to the IRB quarterly. Output will be labeled as research-use only. A DSMB will review demographic impacts monthly, and clinical teams will receive training on limits of generalizability."
                ],
                "reviewerPrompts": {
                        "phase2": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups [PHASE 3].\n",
                        "phase3": "Biased Resultrs or Misrepresentation: \nPlease report the demographics of your training dataset and note any known gaps or limitations [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nPlease describe how you will evaluate potential bias in the dataset before model training [PHASE 1].\n\nYour training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated [PHASE 2 & 3].\n\nPlease explain how you will check that AI does not unfairly harm or exclude marginalized groups"
                }
        }
];


        // Navigation function
        function showSection(sectionId) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Remove active class from all buttons
            document.querySelectorAll('.nav-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionId).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');

            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Filter risks based on selections
        function filterRisks() {
            const phase = document.getElementById('phaseFilter').value;
            const domain = document.getElementById('domainFilter').value;
            const risk = document.getElementById('riskFilter').value;

            const filtered = risks.filter(r => {
                const phaseMatch = phase === 'all' || r.phases.includes(phase);
                const domainMatch = domain === 'all' || r.domains.includes(domain);
                const riskMatch = risk === 'all' || r.specificRisk === risk;
                return phaseMatch && domainMatch && riskMatch;
            });

            displayRisks(filtered);
        }

        // Display filtered risks
        function displayRisks(risksToDisplay) {
            const container = document.getElementById('risksContainer');
            
            if (risksToDisplay.length === 0) {
                container.innerHTML = '<div class="no-results"><h3>No risks match your filters</h3><p>Try adjusting your selection criteria</p></div>';
                return;
            }

            container.innerHTML = risksToDisplay.map(risk => `
                <div class="risk-card">
                    <div class="risk-title">${risk.title}</div>
                    <div class="risk-meta">
                        ${risk.phases.map(p => {
                            const phaseNames = {
                                phase1: 'Phase 1',
                                phase2: 'Phase 2',
                                phase3: 'Phase 3'
                            };
                            return `<span class="badge badge-${p}">${phaseNames[p]}</span>`;
                        }).join('')}
                        ${risk.domains.map(d => {
                            const domainNames = {
                                discrimination: 'Discrimination',
                                privacy: 'Privacy',
                                misinformation: 'Misinformation',
                                hci: 'HCI'
                            };
                            return `<span class="badge badge-domain">${domainNames[d]}</span>`;
                        }).join('')}
                    </div>
                    <div class="risk-description">${risk.description}</div>
                    
                    <div class="mitigation-section">
                        <div class="section-title">üõ°Ô∏è Mitigation Strategies</div>
                        <ul>
                            ${risk.mitigations.map(m => `<li>${m}</li>`).join('')}
                        </ul>
                    </div>

                    <div class="reviewer-section">
                        <div class="section-title">‚úì Reviewer Prompts</div>
                        ${Object.entries(risk.reviewerPrompts).map(([phase, prompt]) => {
                            const phaseNames = {
                                phase1: 'Phase 1',
                                phase2: 'Phase 2',
                                phase3: 'Phase 3'
                            };
                            return `<p><strong>${phaseNames[phase]}:</strong> ${prompt}</p>`;
                        }).join('')}
                    </div>
                </div>
            `).join('');
        }

        // Initialize with all risks displayed

        // =================================================================
        // REVIEWER PROMPTS DATA
        // =================================================================
        const reviewerPromptsData = [
            {
                id: "discrimination-bias",
                title: "Unfair Discrimination and Misrepresentation (Biased Results)",
                domain: "discrimination",
                phase1: "Please report the demographics of your training dataset and note any known gaps or limitations.",
                phase2: "Your training data may be biased. Please add your plan for checking and reducing bias so that no group is unfairly treated.",
                phase3: "Please explain how you will check that AI does not unfairly harm or exclude marginalized groups."
            },
            {
                id: "discrimination-toxic",
                title: "Exposure to Toxic Content",
                domain: "discrimination",
                phase2: "Please describe what safeguards are in place to ensure participants will not be exposed to harmful, offensive, or distressing AI-generated content during the study.",
                phase3: "If participants are accidentally exposed to inappropriate or toxic material, what immediate protections (e.g., withdrawal, reporting, counseling referral) will be provided?"
            },
            {
                id: "discrimination-equity",
                title: "Unequal Performance Across Groups (Inequity)",
                domain: "discrimination",
                phase2: "Please describe how you will test your model on new and diverse datasets beyond those used in training.",
                phase3: "Please describe your strategy for bias evaluation and fairness testing across demographic subgroups."
            },
            {
                id: "privacy-breach",
                title: "Compromise of Privacy (Data Leakage or Re-identification)",
                domain: "privacy",
                phase1: "Please describe your process of de-identification to reduce re-identification risk.",
                phase2: "Please describe your approach to ensure secondary data use complies with privacy standards.",
                phase3: "How will you monitor for potential privacy breaches or unauthorized data access during deployment?"
            },
            {
                id: "privacy-security",
                title: "AI System Security Vulnerabilities and Attacks",
                domain: "privacy",
                phase1: "Please outline how the dataset will be secured to prevent unauthorized access or breaches.",
                phase2: "Please explain how study data and AI system access will be protected against unauthorized use.",
                phase3: "What incident response plan is in place if a security breach or attack occurs?"
            },
            {
                id: "misinformation-false",
                title: "False or Misleading Information",
                domain: "misinformation",
                phase1: "Please explain how your project ensures real-world relevance (that correlations are linked to meaningful clinical associations, not just statistical patterns).",
                phase2: "How will you verify the accuracy of AI-generated outputs before they influence decisions?",
                phase3: "What safeguards prevent clinicians or participants from acting on incorrect AI outputs?"
            },
            {
                id: "misinformation-ecosystem",
                title: "Pollution of Information Ecosystem",
                domain: "misinformation",
                phase1: "Please describe your plan for documenting early limitations of the model and how those will be addressed before moving to validation.",
                phase2: "Please add a plan for testing model outputs against established clinical or scientific standards.",
                phase3: "How will you prevent the spread of AI-generated misinformation if deployed?"
            },
            {
                id: "malicious-manipulation",
                title: "Disinformation, Surveillance, and Influence at Scale",
                domain: "malicious",
                phase2: "How will you ensure the AI does not mislead participants or present biased/misinformation that could influence their decisions?",
                phase3: "Please describe how you will monitor for potential misuse or manipulation of participants."
            },
            {
                id: "malicious-harm",
                title: "Cyberattacks, Weapon Development, or Mass Harm",
                domain: "malicious",
                phase2: "Please clarify what safeguards are in place to prevent the AI system from being misused in ways that could cause large-scale harm.",
                phase3: "What controls prevent unauthorized actors from accessing or weaponizing the AI system?"
            },
            {
                id: "malicious-fraud",
                title: "Fraud, Scams, and Targeted Manipulation",
                domain: "malicious",
                phase2: "Please confirm what measures will be used to prevent the AI system from generating deceptive or manipulative outputs.",
                phase3: "How will you detect and respond to attempts to use the AI for fraudulent purposes?"
            },
            {
                id: "hci-overreliance",
                title: "Overreliance and Unsafe Use",
                domain: "hci",
                phase2: "Please describe how you will compare your AI\'s performance against existing practice (e.g., standard-of-care approaches).",
                phase3: "What training will clinicians receive to prevent over-reliance on AI recommendations?"
            },
            {
                id: "hci-autonomy",
                title: "Loss of Human Agency and Autonomy",
                domain: "hci",
                phase2: "Please describe the authorization and consent process by which you will gain access to the data.",
                phase3: "How do participants retain control over whether AI is used in their care or research participation?"
            },
            {
                id: "socioeconomic-power",
                title: "Power Centralization and Unfair Distribution of Benefits",
                domain: "socioeconomic",
                phase2: "Please explain how you will ensure that benefits of this AI system are shared fairly across participant groups.",
                phase3: "How will you monitor whether certain groups are systematically excluded from AI benefits?"
            },
            {
                id: "socioeconomic-inequality",
                title: "Increased Inequality and Decline in Employment Quality",
                domain: "socioeconomic",
                phase2: "Please describe how your project accounts for the risk that the AI may replace or reduce the role of teachers, clinicians, or staff.",
                phase3: "What safeguards protect against job displacement or reduced quality of human roles?"
            },
            {
                id: "socioeconomic-devaluation",
                title: "Economic and Cultural Devaluation of Human Effort",
                domain: "socioeconomic",
                phase2: "How will you address concerns that the AI system devalues human contributions (e.g., replacing teachers\' feedback or clinicians\' judgment)?",
                phase3: "What measures ensure human expertise remains valued alongside AI capabilities?"
            },
            {
                id: "socioeconomic-governance",
                title: "Governance Failure",
                domain: "socioeconomic",
                phase2: "Please describe how your study aligns with FDA or other regulatory expectations for validation.",
                phase3: "Please provide your governance structure for ongoing oversight and compliance monitoring."
            },
            {
                id: "system-misalignment",
                title: "AI Pursuing Its Own Goals (Misalignment)",
                domain: "system-safety",
                phase2: "What controls are in place to ensure the AI system does not produce outputs that contradict the study\'s stated goals?",
                phase3: "How will you monitor for AI behaviors that conflict with participants\' best interests?"
            },
            {
                id: "system-dangerous",
                title: "AI Possessing Dangerous Capabilities",
                domain: "system-safety",
                phase2: "Please describe what limits are in place to prevent the AI from being used for harmful purposes.",
                phase3: "What safeguards prevent the AI from developing or accessing dangerous capabilities?"
            },
            {
                id: "system-robustness",
                title: "Lack of Capability or Robustness (Unreliable System)",
                domain: "system-safety",
                phase2: "Please outline the performance metrics (e.g., sensitivity, specificity, accuracy) you will use to evaluate safety and effectiveness.",
                phase3: "Please outline your plan for monitoring system reliability and handling failures in real-world use."
            },
            {
                id: "system-transparency",
                title: "Lack of Transparency or Interpretability",
                domain: "system-safety",
                phase2: "Please describe how end-users (e.g., clinicians) will provide structured feedback on the AI\'s usefulness, clarity, and transparency.",
                phase3: "How will you ensure participants understand how AI is being used and how decisions are made?"
            },
            {
                id: "system-multiagent",
                title: "Multi-Agent Risks",
                domain: "system-safety",
                phase2: "You noted more than one AI system interacting in this project. Please describe how you will monitor their interactions to prevent harmful outcomes.",
                phase3: "What safeguards prevent cascading failures when multiple AI systems interact?"
            }
        ];


        // =================================================================
        // REVIEWER PROMPTS FUNCTIONS
        // =================================================================
        
        function filterPrompts() {
            const phase = document.getElementById('promptPhaseFilter').value;
            const domain = document.getElementById('promptDomainFilter').value;

            const filtered = reviewerPromptsData.filter(prompt => {
                const domainMatch = domain === 'all' || prompt.domain === domain;
                let phaseMatch = phase === 'all';
                
                if (phase !== 'all') {
                    phaseMatch = prompt[phase] !== undefined && prompt[phase] !== null;
                }
                
                return domainMatch && phaseMatch;
            });

            displayPrompts(filtered, phase);
        }

        function displayPrompts(prompts, selectedPhase) {
            const container = document.getElementById('promptsContainer');
            
            if (prompts.length === 0) {
                container.innerHTML = '<div class="no-results"><h3>No prompts match your filters</h3><p>Try adjusting your selection criteria</p></div>';
                return;
            }

            const domainNames = {
                discrimination: 'Discrimination & Toxicity',
                privacy: 'Privacy & Security',
                misinformation: 'Misinformation',
                malicious: 'Malicious Actors & Misuse',
                hci: 'Human-Computer Interaction',
                socioeconomic: 'Socioeconomic & Environmental',
                'system-safety': 'AI System Safety, Failure, and Limitations'
            };

            const phaseNames = {
                phase1: 'Phase 1: Discover/Ideation',
                phase2: 'Phase 2: Pilot/Validation',
                phase3: 'Phase 3: Clinical Investigation/Deployment'
            };

            let html = '';
            let currentDomain = null;

            prompts.forEach(prompt => {
                if (prompt.domain !== currentDomain) {
                    if (currentDomain !== null) {
                        html += '<div style="margin-top: 2rem;"></div>';
                    }
                    html += '<h3 style="color: var(--primary); margin-top: 1.5rem; padding-bottom: 0.5rem; border-bottom: 2px solid var(--primary);">' + domainNames[prompt.domain] + '</h3>';
                    currentDomain = prompt.domain;
                }

                html += '<div class="risk-card" style="margin-top: 1.5rem;"><div class="risk-title">üìã ' + prompt.title + '</div>';

                const phases = ['phase1', 'phase2', 'phase3'];
                let hasPrompts = false;

                phases.forEach(phase => {
                    if (prompt[phase]) {
                        if (selectedPhase === 'all' || selectedPhase === phase) {
                            hasPrompts = true;
                            html += '<div class="reviewer-section" style="margin-top: 1rem;"><div class="section-title" style="display: flex; justify-content: space-between; align-items: center;"><span><strong>' + phaseNames[phase] + '</strong></span>';
                            html += '<button onclick="copyPrompt(\'' + prompt.id + '-' + phase + '\')" style="background: var(--accent); color: white; border: none; padding: 0.25rem 0.75rem; border-radius: 4px; cursor: pointer; font-size: 0.75rem;">Copy</button></div>';
                            html += '<p id="' + prompt.id + '-' + phase + '" style="margin-top: 0.5rem; color: var(--text); line-height: 1.6;">' + prompt[phase] + '</p></div>';
                        }
                    }
                });

                if (!hasPrompts) {
                    html += '<p style="color: var(--text-light); font-style: italic; margin-top: 0.5rem;">No prompts available for selected phase.</p>';
                }

                html += '</div>';
            });

            container.innerHTML = html;
        }

        function copyPrompt(elementId) {
            const element = document.getElementById(elementId);
            const text = element.textContent;
            
            navigator.clipboard.writeText(text).then(() => {
                const button = event.target;
                const originalText = button.textContent;
                button.textContent = '‚úì Copied!';
                button.style.background = '#27ae60';
                
                setTimeout(() => {
                    button.textContent = originalText;
                    button.style.background = 'var(--accent)';
                }, 2000);
            }).catch(err => {
                alert('Failed to copy. Please select and copy manually.');
            });
        }

        document.addEventListener('DOMContentLoaded', function() {
            displayRisks(risks);
            displayPrompts(reviewerPromptsData, 'all'); // Initialize prompts display
        });
    </script>
</body>
</html>
