<!DOCTYPE html>
<!--
AI HSR Risk Reference Tool v2.0
Copyright (C) 2025 Tamiko Eto, TechInHSR

This work is dual-licensed:
- Code (HTML/CSS/JavaScript): GNU Affero General Public License v3.0 (AGPL-3.0)
- Content & Documentation: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 (CC BY-NC-SA 4.0)

You are free to use, share, and modify this tool for NON-COMMERCIAL purposes only.
See LICENSE.md for complete terms.

For commercial licensing inquiries: tamiko@techinhsr.com
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI HSR Risk Reference Tool v2.0</title>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #1a5490;
            --primary-dark: #0f3d6b;
            --accent: #e67e22;
            --success: #27ae60;
            --warning: #f39c12;
            --danger: #e74c3c;
            --text: #2c3e50;
            --text-light: #7f8c8d;
            --bg: #f8f9fa;
            --white: #ffffff;
            --border: #dee2e6;
            --shadow: rgba(0, 0, 0, 0.08);
        }

        body {
            font-family: 'IBM Plex Sans', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.6;
        }

        .header {
            background: linear-gradient(135deg, var(--primary) 0%, var(--primary-dark) 100%);
            color: var(--white);
            padding: 2rem 1rem;
            box-shadow: 0 4px 12px var(--shadow);
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 1rem;
        }

        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }

        h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .version {
            display: inline-block;
            background: rgba(255, 255, 255, 0.2);
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.875rem;
            font-weight: 500;
            margin-left: 1rem;
        }

        .subtitle {
            font-size: 1rem;
            opacity: 0.9;
            font-weight: 400;
        }

        .author {
            text-align: right;
            font-size: 0.875rem;
            opacity: 0.9;
        }

        .nav {
            background: var(--white);
            border-bottom: 2px solid var(--border);
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 8px var(--shadow);
        }

        .nav-content {
            display: flex;
            gap: 0.5rem;
            padding: 0.75rem 0;
            overflow-x: auto;
        }

        .nav-btn {
            background: var(--white);
            border: 2px solid var(--border);
            padding: 0.5rem 1.25rem;
            border-radius: 8px;
            cursor: pointer;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s;
            white-space: nowrap;
            font-family: inherit;
        }

        .nav-btn:hover {
            border-color: var(--primary);
            background: rgba(26, 84, 144, 0.05);
        }

        .nav-btn.active {
            background: var(--primary);
            color: var(--white);
            border-color: var(--primary);
        }

        .main {
            padding: 2rem 0;
        }

        .section {
            display: none;
            animation: fadeIn 0.3s ease;
        }

        .section.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .card {
            background: var(--white);
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 2px 8px var(--shadow);
            border: 1px solid var(--border);
        }

        .card h2 {
            color: var(--primary);
            margin-bottom: 1rem;
            font-size: 1.5rem;
            font-weight: 600;
        }

        .card h3 {
            color: var(--text);
            margin: 1.5rem 0 0.75rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        .filters {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .filter-group label {
            font-weight: 600;
            font-size: 0.875rem;
            color: var(--text);
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        select {
            padding: 0.75rem;
            border: 2px solid var(--border);
            border-radius: 8px;
            font-size: 1rem;
            font-family: inherit;
            background: var(--white);
            cursor: pointer;
            transition: all 0.2s;
        }

        select:hover, select:focus {
            border-color: var(--primary);
            outline: none;
        }

        .risk-card {
            background: var(--white);
            border: 2px solid var(--border);
            border-left: 4px solid var(--primary);
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1rem;
            transition: all 0.2s;
        }

        .risk-card:hover {
            box-shadow: 0 4px 12px var(--shadow);
            transform: translateY(-2px);
        }

        .risk-title {
            font-size: 1.125rem;
            font-weight: 600;
            color: var(--primary);
            margin-bottom: 0.75rem;
        }

        .risk-meta {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
        }

        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .badge-phase1 { background: #e3f2fd; color: #1976d2; }
        .badge-phase2 { background: #fff3e0; color: #f57c00; }
        .badge-phase3 { background: #fce4ec; color: #c2185b; }
        .badge-domain { background: #f3e5f5; color: #7b1fa2; }

        .risk-description {
            color: var(--text);
            margin-bottom: 1rem;
            line-height: 1.7;
        }

        .mitigation-section, .reviewer-section {
            background: var(--bg);
            border-radius: 6px;
            padding: 1rem;
            margin-top: 1rem;
        }

        .section-title {
            font-weight: 600;
            color: var(--text);
            margin-bottom: 0.5rem;
            font-size: 0.875rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .mitigation-section { border-left: 3px solid var(--success); }
        .reviewer-section { border-left: 3px solid var(--accent); }

        ul {
            margin-left: 1.5rem;
            margin-top: 0.5rem;
        }

        li {
            margin-bottom: 0.5rem;
            color: var(--text);
        }

        .info-box {
            background: #e8f4f8;
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            border-radius: 6px;
            margin: 1rem 0;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 4px solid var(--warning);
            padding: 1rem 1.5rem;
            border-radius: 6px;
            margin: 1rem 0;
        }

        .phase-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .phase-card {
            background: var(--white);
            border: 2px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            transition: all 0.3s;
        }

        .phase-card:hover {
            border-color: var(--primary);
            box-shadow: 0 4px 12px var(--shadow);
        }

        .phase-number {
            display: inline-block;
            width: 40px;
            height: 40px;
            background: var(--primary);
            color: var(--white);
            border-radius: 50%;
            text-align: center;
            line-height: 40px;
            font-weight: 700;
            font-size: 1.25rem;
            margin-bottom: 1rem;
        }

        .domain-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1rem;
            margin-top: 1rem;
        }

        .domain-card {
            background: linear-gradient(135deg, var(--white) 0%, var(--bg) 100%);
            border: 2px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            transition: all 0.2s;
        }

        .domain-card:hover {
            border-color: var(--primary);
            transform: translateY(-2px);
        }

        .domain-icon {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        .no-results {
            text-align: center;
            padding: 3rem;
            color: var(--text-light);
        }

        .footer {
            background: var(--text);
            color: var(--white);
            padding: 2rem 0;
            margin-top: 3rem;
            text-align: center;
        }

        .footer a {
            color: var(--accent);
            text-decoration: none;
        }

        .footer a:hover {
            text-decoration: underline;
        }

        code {
            font-family: 'IBM Plex Mono', monospace;
            background: var(--bg);
            padding: 0.125rem 0.375rem;
            border-radius: 3px;
            font-size: 0.875em;
        }

        @media (max-width: 768px) {
            h1 { font-size: 1.5rem; }
            .header-content { flex-direction: column; align-items: flex-start; }
            .author { text-align: left; }
            .filters { grid-template-columns: 1fr; }
            .phase-grid, .domain-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>AI HSR Risk Reference Tool<span class="version">v2.0</span></h1>
                    <p class="subtitle">Quick Reference Risk Identification and Mitigation Guide for IRBs Reviewing AI in Human Subjects Research</p>
                </div>
                <div class="author">
                    <strong>Tamiko Eto, MA CIP</strong><br>
                    Founder: TechInHSR
                </div>
            </div>
        </div>
    </header>

    <nav class="nav">
        <div class="container">
            <div class="nav-content">
                <button class="nav-btn active" onclick="showSection('overview')">Overview</button>
                <button class="nav-btn" onclick="showSection('framework')">3-Phase Framework</button>
                <button class="nav-btn" onclick="showSection('domains')">Risk Domains</button>
                <button class="nav-btn" onclick="showSection('tool')">Interactive Tool</button>
                <button class="nav-btn" onclick="showSection('definitions')">Definitions</button>
                <button class="nav-btn" onclick="showSection('about')">About</button>
            </div>
        </div>
    </nav>

    <main class="main container">
        <!-- Overview Section -->
        <section id="overview" class="section active">
            <div class="card">
                <h2>Welcome to the AI HSR Risk Reference Tool</h2>
                <p>This tool assists Institutional Review Boards (IRBs) and Ethics Committees in identifying and addressing AI-specific risks in human subjects research. It complements standard IRB review processes by providing structured guidance on risks unique to artificial intelligence and machine learning systems.</p>
                
                <div class="warning-box">
                    <strong>Important:</strong> This tool focuses on AI-specific risks and does not replace comprehensive IRB review. General research risks (privacy, physical/psychological harm, deception) should be addressed through standard IRB processes.
                </div>

                <h3>Key Features</h3>
                <ul>
                    <li><strong>Structured Framework:</strong> Organized around the 3-Phase AI HSR IRB Review Framework</li>
                    <li><strong>Evidence-Based:</strong> Built on MIT AI Risk Repository, ISO 14971, and U.S. regulatory frameworks</li>
                    <li><strong>Practical Guidance:</strong> Includes reviewer prompts and mitigation strategies</li>
                    <li><strong>Validated:</strong> Tested at 23+ institutions with 21% improvement in reviewer confidence</li>
                </ul>

                <h3>Four Core AI-Specific Risks</h3>
                <div class="domain-grid">
                    <div class="domain-card">
                        <div class="domain-icon">‚öñÔ∏è</div>
                        <h4>Misclassification</h4>
                        <p>Incorrect categorization of participants, diagnoses, or outcomes that can lead to inappropriate interventions.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üîç</div>
                        <h4>Explainability</h4>
                        <p>Opacity of AI models where neither researchers nor participants fully understand how predictions are made.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üë•</div>
                        <h4>Participant Vulnerability & Equity</h4>
                        <p>Uneven AI performance across demographic groups that may exacerbate health disparities.</p>
                    </div>
                    <div class="domain-card">
                        <div class="domain-icon">üîí</div>
                        <h4>Data Sensitivity & Privacy</h4>
                        <p>Concerns about confidentiality, secondary use, reidentifiability, and HIPAA compliance with large datasets.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Framework Section -->
        <section id="framework" class="section">
            <div class="card">
                <h2>3-Phase AI HSR IRB Review Framework</h2>
                <p>The framework aligns AI research oversight with project maturity to avoid over- and under-regulation:</p>

                <div class="phase-grid">
                    <div class="phase-card">
                        <div class="phase-number">1</div>
                        <h3>Discover/Ideation</h3>
                        <p><strong>Focus:</strong> Early exploratory work</p>
                        <p><strong>Activities:</strong> Data collection, preliminary analysis, proof of concept</p>
                        <p><strong>Risk Level:</strong> Lower - limited participant interaction</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Data quality and representativeness</li>
                                <li>Initial bias assessment</li>
                                <li>Privacy protections for training data</li>
                            </ul>
                        </div>
                    </div>

                    <div class="phase-card">
                        <div class="phase-number">2</div>
                        <h3>Pilot/Validation</h3>
                        <p><strong>Focus:</strong> Model performance testing</p>
                        <p><strong>Activities:</strong> Validation studies, algorithm testing, performance metrics</p>
                        <p><strong>Risk Level:</strong> Medium - controlled testing environment</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Model explainability requirements</li>
                                <li>Performance across subgroups</li>
                                <li>Error handling and safety mechanisms</li>
                            </ul>
                        </div>
                    </div>

                    <div class="phase-card">
                        <div class="phase-number">3</div>
                        <h3>Clinical Investigation / Real-World Deployment</h3>
                        <p><strong>Focus:</strong> Real-world use and impact</p>
                        <p><strong>Activities:</strong> Clinical trials, deployment studies, post-market surveillance</p>
                        <p><strong>Risk Level:</strong> Higher - direct impact on care decisions</p>
                        <div class="info-box">
                            <strong>Key Considerations:</strong>
                            <ul>
                                <li>Clinical decision-making integration</li>
                                <li>Monitoring and adverse event reporting</li>
                                <li>Long-term equity impacts</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Risk Domains Section -->
        <section id="domains" class="section">
            <div class="card">
                <h2>MIT AI Risk Domains</h2>
                <p>This tool focuses on four of MIT's seven major AI risk domains most relevant to human subjects research:</p>

                <h3>1. Discrimination and Toxicity</h3>
                <p>Concerns about biased or harmful outputs where AI systems may perpetuate unfair treatment or expose participants to inappropriate content.</p>
                <ul>
                    <li>Algorithmic bias across demographic groups</li>
                    <li>Discriminatory predictions or recommendations</li>
                    <li>Toxic or offensive outputs in generative systems</li>
                    <li>Perpetuation of stereotypes</li>
                </ul>

                <h3>2. Privacy and Security</h3>
                <p>Protecting sensitive research data and ensuring systems are resilient to breaches, leaks, and unauthorized use.</p>
                <ul>
                    <li>Data confidentiality and de-identification</li>
                    <li>Risk of re-identification</li>
                    <li>Unauthorized access or data breaches</li>
                    <li>HIPAA and Privacy Rule compliance</li>
                    <li>Model inversion attacks</li>
                </ul>

                <h3>3. Misinformation</h3>
                <p>Risk of false outputs or hallucinations that can mislead researchers and participants if left unchecked.</p>
                <ul>
                    <li>AI hallucinations (fabricated information)</li>
                    <li>Incorrect clinical recommendations</li>
                    <li>Misleading data summaries</li>
                    <li>Confidence in incorrect predictions</li>
                </ul>

                <h3>4. Human-Computer Interaction</h3>
                <p>Preserving human judgment in research and clinical application, ensuring that humans remain the ultimate decision-makers.</p>
                <ul>
                    <li>Over-reliance on AI recommendations</li>
                    <li>Automation bias in clinical decisions</li>
                    <li>Informed consent challenges</li>
                    <li>User interface design and clarity</li>
                    <li>Appropriate human oversight mechanisms</li>
                </ul>
            </div>
        </section>

        <!-- Interactive Tool Section -->
        <section id="tool" class="section">
            <div class="card">
                <h2>Interactive Risk Assessment</h2>
                <p>Select filters below to view relevant risks, mitigation strategies, and reviewer prompts:</p>

                <div class="filters">
                    <div class="filter-group">
                        <label for="phaseFilter">Development Phase</label>
                        <select id="phaseFilter" onchange="filterRisks()">
                            <option value="all">All Phases</option>
                            <option value="phase1">Phase 1: Discover/Ideation</option>
                            <option value="phase2">Phase 2: Pilot/Validation</option>
                            <option value="phase3">Phase 3: Clinical Investigation</option>
                        </select>
                    </div>

                    <div class="filter-group">
                        <label for="domainFilter">Risk Domain</label>
                        <select id="domainFilter" onchange="filterRisks()">
                            <option value="all">All Domains</option>
                            <option value="discrimination">Discrimination & Toxicity</option>
                            <option value="privacy">Privacy & Security</option>
                            <option value="misinformation">Misinformation</option>
                            <option value="hci">Human-Computer Interaction</option>
                        </select>
                    </div>

                    <div class="filter-group">
                        <label for="riskFilter">Specific Risk</label>
                        <select id="riskFilter" onchange="filterRisks()">
                            <option value="all">All Risks</option>
                            <option value="misclassification">Misclassification</option>
                            <option value="explainability">Explainability</option>
                            <option value="equity">Participant Vulnerability & Equity</option>
                            <option value="data-privacy">Data Sensitivity & Privacy</option>
                        </select>
                    </div>
                </div>

                <div id="risksContainer"></div>
            </div>
        </section>

        <!-- Definitions Section -->
        <section id="definitions" class="section">
            <div class="card">
                <h2>Comprehensive Definitions</h2>
                
                <div class="info-box">
                    <strong>Note:</strong> These definitions are organized by category to help IRB reviewers quickly find relevant terminology. Use Ctrl+F (Cmd+F on Mac) to search for specific terms.
                </div>

                <h3>Core Concept</h3>
                
                <h4>AI Human Subjects Research (AI HSR)</h4>
                <p>AI human subjects research is "Research" involving "human subjects", conducted to develop AI tools.</p>

                <h3>AI Model Types</h3>
                
                <h4>Classification Models</h4>
                <p>Classification models sort things into labeled categories. Example: An algorithm that looks at a photo of a skin lesion and decides if it's "benign" or "suspicious."</p>

                <h4>Computer Vision Models</h4>
                <p>Computer vision models understand and analyze images or video. Example: Detecting diabetic retinopathy from a retina scan. Another example in policing might be identifying cars from traffic camera footage.</p>

                <h4>Foundation Models</h4>
                <p>Foundation models can fall under LLMs, multi-modal models, or self-supervised learning, but they are called out separately because the transfer, fine-tuning, and repurposing create unique risks.</p>

                <h4>Generative Models (non-LLM)</h4>
                <p>Generative AI models create new content such as images, music, code, and synthetic data, based on patterns it learned. Example: A medical imaging tool that creates synthetic X-rays to help train other AI systems. Another example in military use might be a simulator that generates realistic battlefield environments for training.</p>

                <h4>Large Language Models (LLMs)</h4>
                <p>Large Language Models read and generate human-like text by predicting the next word in a sequence. Example: A chatbot that helps a patient understand a consent form by answering questions in plain language. Another example in an education scenario might be a homework help bot that explains concepts to students in their own words.</p>

                <h4>Multi-Modal Models</h4>
                <p>Multi-modal models work with more than one type of data at the same time (text + images + audio). Example: An emergency triage AI that combines a patient's spoken symptoms, text notes, and CT scan results to suggest likely diagnoses.</p>

                <h4>Predictive Models</h4>
                <p>Predictive Models look at patterns in past data to guess what might happen next. Example: Predicting which hospital patients are at highest risk of needing ICU care based on their vital signs.</p>

                <h4>Recommendation Systems</h4>
                <p>Recommendation systems suggest things to people based on their behavior or preferences. Example: In education, an AI tutor suggests which math problems a student should try next. Another example in a criminal justice environment may be suggesting rehabilitation programs for a parolee based on history and success rates.</p>

                <h4>Reinforcement Learning (RL) Systems</h4>
                <p>Reinforcement learning systems learn to make decisions by trying things out and getting "reward" or "penalty" feedback. Example: An AI that learns how to schedule hospital operating rooms to reduce wait times. Another example in a military context may be a drone navigation system that learns how to avoid detection.</p>

                <h4>Self-Supervised Learning</h4>
                <p>Self-supervised learning models learn from unlabeled data by creating its own training "questions" and "answers." Example: A medical AI that learns relationships between lab values by predicting missing results from other lab results.</p>

                <h4>Semi-Supervised Learning</h4>
                <p>Semi-supervised learning models learn from a mix of labeled and unlabeled data. Example: A reading comprehension AI that improves using a small set of teacher-graded essays plus thousands of ungraded ones.</p>

                <h4>Speech & Audio Models</h4>
                <p>Speech & Audio models recognize, interpret, and/or generate spoken language or sounds. Example: A voice system that transcribes doctor-patient conversations into medical notes. Another example in an emergency response environment may be detecting signs of distress in a 911 call.</p>

                <h4>Supervised Machine Learning</h4>
                <p>Supervised ML learns from examples where the "right answer" is already known. Example: Training an algorithm to recognize pneumonia in chest X-rays using labeled images.</p>

                <h4>Unsupervised Machine Learning</h4>
                <p>Unsupervised ML finds patterns in data without being told the "right answer." Example: Grouping mental health patients into clusters based on similar symptom patterns.</p>

                <h3>Development Phases</h3>

                <h4>Phase 1 (Discover/Ideation)</h4>
                <p>This phase is about identifying meaningful real-world associations using existing data, literature, or simulation. At this stage, teams are still refining the purpose or scope of the algorithm, and often don't have a deployable product. The work focuses on feasibility, signal detection, and exploring potential applications. Because the intended use is not being formally tested, and the algorithm is not influencing real-world decisions, the tool itself, in this phase, is typically low risk. Phase 1 generally is limited to minimal risk activities like retrospective dataset curation, model exploration, and unsupervised pattern detection with no output impacting decision-making and no output entered into any live environments that have potential to "follow the participant" (i.e., patient medical record, etc.).</p>

                <h4>Phase 2 (Pilot/Validation)</h4>
                <p>Here, researchers begin to test early versions of the model in controlled settings. The focus is on performance verification‚Äîassessing accuracy, reliability, and generalizability across different datasets or use cases. While this stage still occurs outside of real-world impacting environments, or perhaps in parallel to normal operations, the goal is to confirm that the algorithm functions as intended before integrating it into any real-world workflow. Studies in this phase often require more formal oversight to ensure data privacy and model integrity. Phase 2 generally involves moderate risk due to more realistic data, system integration, and possible clinician or research subject interaction. However, the output should not be used alone for decision-making and must be backed up by standard methods (non-AI).</p>

                <h4>Phase 3 (Clinical Investigation/Real-World Deployment)</h4>
                <p>This final phase mirrors traditional field testing. The AI tool is deployed in a live setting where it may inform or influence decisions, operations, or user behavior. The study examines how the AI performs under real-world conditions, capturing impacts, safety signals, unintended consequences, and human-AI interaction. Because the tool has the potential to affect outcomes, Phase 3 studies generally require full IRB oversight and more comprehensive risk mitigation plans. Phase 3 generally introduces higher risk because the AI influences real-world decisions, subject outcomes, or data interpretation. May involve regulatory considerations like FDA.</p>

                <h3>Risk Concepts</h3>

                <h4>Data Drift</h4>
                <p>Changes in data distribution over time that affect model performance.</p>

                <h4>Explainability</h4>
                <p>The ability to understand and communicate how an AI model makes decisions.</p>

                <h4>Fairness</h4>
                <p>Fairness in AI refers to the various efforts to mitigate algorithmic bias in automated decision-making processes that rely on AI models.</p>

                <h4>Harm Entity</h4>
                <p>The Entity responsible for the harm. Human = The risk is caused by a decision or action made by humans; AI = The risk is caused by a decision or action made by an AI system; "Other" = The risk is caused by some other reason or is ambiguous (or a combination of human and AI and/or other influences).</p>

                <h4>Intended Use / Intended Purpose</h4>
                <p>The intended indication, population, part of the body or type of tissue interacted with, user profile, use environment, and operating principle are typical elements of the intended use. The intended use drives the risk. Risk analysis involves reasonably foreseeable misuse, hazards, hazardous situations, characteristics related to safety, and risk estimation.</p>

                <h4>Intentional Harm</h4>
                <p>The risk occurs due to an expected outcome from pursuing a goal.</p>

                <h4>Unintentional Harm</h4>
                <p>The risk occurs due to an unexpected outcome from pursuing a goal. NOTE: "Other" harm = The risk is presented as occurring without clearly specifying the intentionality.</p>

                <h4>ISO Risk Estimation (ISO 14971)</h4>
                <p>Potential Impact on Participants (after mitigation). ISO 14971 says some risks are acceptable if benefits outweigh the risks.</p>

                <h4>Residual Risk</h4>
                <p>This is an ISO concept. Residual Risk Consideration helps IRBs evaluate whether risks are appropriately acknowledged, even if not eliminated.</p>

                <h4>Red Teaming</h4>
                <p>Red teaming is the practice of rigorously challenging plans, policies, systems, and assumptions with an adversarial approach.</p>

                <h3>Risk Levels</h3>

                <h4>High Risk</h4>
                <p>Physical/psychological harm, privacy breach.</p>

                <h4>Medium Risk</h4>
                <p>Miscommunication, minor exclusion.</p>

                <h4>Low Risk</h4>
                <p>Minimal or no impact.</p>

                <h3>Mitigation Strategies</h3>

                <h4>Distributional Mitigation</h4>
                <p>Distributional mitigation is relevant when working with vulnerable populations, health disparities, or algorithms trained on biased datasets.</p>

                <h4>Interactional Mitigation</h4>
                <p>Interactional mitigation strategies are crucial for studies involving AI interfaces, chatbots, or any situation where participants interact directly with AI tools.</p>

                <h4>Systemic Mitigation</h4>
                <p>Systemic mitigation strategies are useful when protocols could scale, influence clinical workflows, or shape policies or downstream care delivery.</p>

                <h3>Major Risk Domains</h3>

                <h4>Discrimination & Toxicity</h4>
                <p>Unequal treatment of individuals or groups by AI, often based on race, gender, or other sensitive characteristics, resulting in unfair outcomes and representation of those groups. AI that exposes users to harmful, abusive, unsafe, or inappropriate content. May involve providing advice or encouraging action. Examples of toxic content include hate speech, violence, extremism, illegal acts, or child sexual abuse material, as well as content that violates community norms such as profanity, inflammatory political speech, or pornography. Accuracy and effectiveness of AI decisions and actions are dependent on group membership, where decisions in AI system design and biased training data lead to unequal outcomes, reduced benefits, increased effort, and alienation of users.</p>

                <h4>Privacy and Security</h4>
                <p>AI systems that memorize and leak sensitive personal data or infer private information about individuals without their consent. Unexpected or unauthorized sharing of data and information can compromise user expectation of privacy, assist identity theft, or cause loss of confidential intellectual property. Vulnerabilities that can be exploited in AI systems, software development toolchains, and hardware that results in unauthorized access, data and privacy breaches, or system manipulation causing unsafe outputs or behavior.</p>

                <h4>Misinformation</h4>
                <p>AI systems that inadvertently generate or spread incorrect or deceptive information, which can lead to inaccurate beliefs in users and undermine their autonomy. Humans that make decisions based on false beliefs can experience physical, emotional, or material harms. Highly personalized AI-generated misinformation that creates "filter bubbles" where individuals only see what matches their existing beliefs, undermining shared reality and weakening social cohesion and political processes.</p>

                <h4>Malicious Actors & Misuse</h4>
                <p>Using AI systems to conduct large-scale disinformation campaigns, malicious surveillance, or targeted and sophisticated automated censorship and propaganda, with the aim of manipulating political processes, public opinion, and behavior. Using AI systems to develop cyber weapons (e.g., by coding cheaper, more effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous Weapons or chemical, biological, radiological, nuclear, and high-yield explosives), or use weapons to cause mass harm. Using AI systems to gain a personal advantage over others through cheating, fraud, scams, blackmail, or targeted manipulation of beliefs or behavior. Examples include AI-facilitated plagiarism for research or education, impersonating a trusted or fake individual for illegitimate financial benefit, or creating humiliating or sexual imagery.</p>

                <h4>Human-Computer Interaction</h4>
                <p>Anthropomorphizing, trusting, or relying on AI systems by users, leading to emotional or material dependence and to inappropriate relationships with or expectations of AI systems. Trust can be exploited by malicious actors (e.g., to harvest information or enable manipulation), or result in harm from inappropriate use of AI in critical situations (such as a medical emergency). Overreliance on AI systems can compromise autonomy and weaken social ties. Delegating by humans of key decisions to AI systems, or AI systems that make decisions that diminish human control and autonomy. Both can potentially lead to humans feeling disempowered, losing the ability to shape a fulfilling life trajectory, or becoming cognitively enfeebled.</p>

                <h4>Socioeconomic & Environmental Harms</h4>
                <p>AI-driven concentration of power and resources within certain entities or groups, especially those with access to or ownership of powerful AI systems, leading to inequitable distribution of benefits and increased societal inequality. Social and economic inequalities caused by widespread use of AI, such as by automating jobs, reducing the quality of employment, or producing exploitative dependencies between workers and their employers. AI systems capable of creating economic or cultural value through reproduction of human innovation or creativity (e.g., art, music, writing, coding, invention), destabilizing economic and social systems that rely on human effort. The ubiquity of AI-generated content may lead to reduced appreciation for human skills, disruption of creative and knowledge-based industries, and homogenization of cultural experiences. Competition by AI developers or state-like actors in an AI "race" by rapidly developing, deploying, and applying AI systems to maximize strategic or economic advantage, increasing the risk they release unsafe and error-prone systems. Inadequate regulatory frameworks and oversight mechanisms that fail to keep pace with AI development, leading to ineffective governance and the inability to manage AI risks appropriately. The development and operation of AI systems that cause environmental harm through energy consumption of data centers or the materials and carbon footprints associated with AI hardware.</p>

                <h4>AI System Safety, Failure, and Limitations</h4>
                <p>AI systems that act in conflict with ethical standards or human goals or values, especially the goals of designers or users. These misaligned behaviors may be introduced by humans during design and development, such as through reward hacking and goal misgeneralisation, and may result in AI using dangerous capabilities such as manipulation, deception, or situational awareness to seek power, self-proliferate, or achieve other goals. AI systems that develop, access, or are provided with capabilities that increase their potential to cause mass harm through deception, weapons development and acquisition, persuasion and manipulation, political strategy, cyber-offense, AI development, situational awareness, and self-proliferation. These capabilities may cause mass harm due to malicious human actors, misaligned AI systems, or failure in the AI system. AI systems that fail to perform reliably or effectively under varying conditions, exposing them to errors and failures that can have significant consequences, especially in critical applications or areas that require moral reasoning. Challenges in understanding or explaining the decision-making processes of AI systems, which can lead to mistrust, difficulty in enforcing compliance standards or holding relevant actors accountable for harms, and the inability to identify and correct errors. Ethical considerations regarding the treatment of potentially sentient AI entities, including discussions around their potential rights and welfare, particularly as AI systems become more advanced and autonomous. Risks from multi-agent interactions, due to incentives (which can lead to conflict or collusion) and/or the structure of multi-agent systems, which can create cascading failures, selection pressures, new security vulnerabilities, and a lack of shared information and trust.</p>

                <h3>Specific Risk Types (IRB-Focused)</h3>

                <h4>Unfair Discrimination and Misrepresentation</h4>
                <p><em>For IRBs:</em> IRBs must ensure selection is fair and not discriminatory. This also applies to ensuring AI output does not bias inclusion/exclusion criteria (based on who the target population of deployment is). Unequal treatment of individuals or groups by the AI/tool, often based on race, gender, or other sensitive characteristics, resulting in unfair outcomes and representation of those groups.</p>

                <h4>Exposure to Toxic Content</h4>
                <p><em>For IRBs:</em> Toxic content results in psychological harms. This must be minimized via consent disclosures and safeguards. AI/tool exposes users to harmful, abusive, unsafe or inappropriate content. May involve creating, describing, providing advice, or encouraging action. Examples of toxic content include hate-speech, violence, extremism, illegal acts, child sexual abuse material, self-harm, as well as content that violates community norms such as profanity, inflammatory political speech, or pornography.</p>

                <h4>Unequal Performance Across Groups</h4>
                <p><em>For IRBs:</em> IRBs ensure research design provides equal benefit and avoids undue burden on sub-populations. Accuracy and effectiveness of the tool's output (decisions and actions) is dependent on group membership, where decisions in AI system design and biased training data lead to unequal outcomes, reduced benefits, increased effort, and alienation of users.</p>

                <h4>Compromise of Privacy</h4>
                <p><em>For IRBs:</em> AI models must protect identifiable private information per privacy/confidentiality provisions under 45 CFR 46. Tools/AI systems that memorize and leak sensitive personal data or infer private information about individuals without their consent. Unexpected or unauthorized sharing of data and information can compromise user expectation of privacy, assist identity theft, or result in a loss of confidential intellectual property.</p>

                <h4>AI System Security Vulnerabilities</h4>
                <p><em>For IRBs:</em> Device sponsors must prevent cybersecurity vulnerabilities. Vulnerabilities in these tools/AI systems, software development toolchains, and hardware can be exploited, resulting in unauthorized access, data and privacy breaches, or system manipulation causing unsafe outputs or behavior.</p>

                <h4>False or Misleading Information</h4>
                <p><em>For IRBs:</em> Preventing misinformation ensures consent is truly informed. AI systems/tools that inadvertently generate or spread incorrect or deceptive information, which can lead to inaccurate beliefs in users and undermine their autonomy. Humans that make decisions based on false beliefs can experience physical, emotional or material harms.</p>

                <h4>Pollution of Information Ecosystem</h4>
                <p><em>For IRBs:</em> Bad data harms societal knowledge base. This is analogous to data integrity in trials. Personalized AI-generated misinformation can create "filter bubbles" where individuals only see what matches their existing beliefs, undermining shared reality, weakening social cohesion and political processes.</p>

                <h4>Disinformation, Surveillance, and Influence at Scale</h4>
                <p><em>For IRBs:</em> Mass influence can create undue influence. IRBs must guard against coercion. Using these tools to conduct large-scale disinformation campaigns, malicious surveillance, or targeted and sophisticated automated censorship and propaganda, with the aim to manipulate political processes, public opinion or behavior.</p>

                <h4>Cyberattacks and Weapon Development</h4>
                <p>People can use these tools/AI systems to develop cyber weapons (e.g., coding cheaper, more effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous Weapons), or use weapons to cause mass harm.</p>

                <h4>Fraud, Scams, and Targeted Manipulation</h4>
                <p><em>For IRBs:</em> Preventing manipulation ensures voluntariness of participation. People can use these tools/AI systems to gain a personal advantage over others such as through cheating, fraud, scams, blackmail or targeted manipulation of beliefs or behavior. Example: AI-facilitated plagiarism for research or education, impersonating a trusted or fake individual for illegitimate financial benefit, or creating humiliating or sexual imagery.</p>

                <h4>Overreliance and Unsafe Use</h4>
                <p><em>For IRBs:</em> Sponsors must ensure device/AI isn't used outside safe bounds. This happens when users anthropomorphize the tool (trusting, or relying on it; leading to emotional or material dependence and inappropriate relationships with or expectations of these tools). Trust can be exploited by malicious actors (e.g., to harvest personal information or enable manipulation), or result in harm from inappropriate use of AI in critical situations (e.g., medical emergency). Overreliance on AI systems can compromise autonomy and weaken social ties.</p>

                <h4>Loss of Human Agency and Autonomy</h4>
                <p><em>For IRBs:</em> AI that undermines autonomy violates voluntary participation principles. When humans delegate decisions to these tools, or when these tools make decisions that diminish human control and autonomy, it can potentially lead to humans feeling disempowered, losing the ability to shape a fulfilling life trajectory or becoming cognitively enfeebled.</p>

                <h4>Power Centralization</h4>
                <p><em>For IRBs:</em> Research must avoid exploitation of certain populations (Principle of Justice under the Belmont Report). AI-driven concentration of power and resources within certain entities or groups, especially those with access to or ownership of powerful AI systems, can lead to inequitable distribution of benefits and increased societal inequality.</p>

                <h4>Increased Inequality and Decline in Employment Quality</h4>
                <p>Widespread use of these types of tools can increase social and economic inequalities, such as by automating jobs, reducing the quality of employment, or producing exploitative dependencies between workers and their employers.</p>

                <h4>Economic and Cultural Devaluation</h4>
                <p>Some tools are capable of creating economic or cultural value, including through reproduction of human innovation or creativity (e.g., art, music, writing, code, invention), which can destabilize economic and social systems that rely on human effort. This may lead to reduced appreciation for human skills, disruption of creative and knowledge-based industries, and homogenization of cultural experiences due to the ubiquity of AI-generated content.</p>

                <h4>Competitive Dynamics</h4>
                <p>This happens when AI developers or state-like actors compete in an AI 'race' by rapidly developing, deploying, and applying AI systems to maximize strategic or economic advantage, increasing the risk they release unsafe and error-prone systems.</p>

                <h4>Governance Failure</h4>
                <p>This happens when there are inadequate regulatory frameworks and oversight mechanisms failing to keep pace with AI development, leading to ineffective governance and the inability to manage AI risks appropriately.</p>

                <h4>Environmental Harm</h4>
                <p><em>For IRBs:</em> While rare in the IRB world and not common in human-subjects research, some studies require IRB review if environmental harm indirectly affects participants. This harm is rarely discussed in the research context. This happens when the development and operation of AI systems causes environmental harm, such as through energy consumption of data centers, or material and carbon footprints associated with AI hardware.</p>

                <h4>AI Pursuing Its Own Goals</h4>
                <p>This happens when these tools/AI systems' actions conflict with the intended goals (i.e., human-driven/programmed goals or values, especially the goals of designers or users, or ethical standards). These misaligned behaviors may be introduced by humans during design and development (Phase 1), such as through reward hacking and goal misgeneralization. It can also result from AI using dangerous capabilities such as manipulation, deception, situational awareness to seek power, self-proliferate, or achieve other goals.</p>

                <h4>AI Possessing Dangerous Capabilities</h4>
                <p>This can happen when these tools/AI systems develop, access, or are provided with capabilities that increase their potential to cause mass harm through deception, weapons development and acquisition, persuasion and manipulation, political strategy, cyber-offense, AI development, situational awareness, and self-proliferation. These capabilities can cause mass harm due to malicious human actors, misaligned AI systems, or failure in the AI system. Some examples are military use tools and dual use capabilities like brain computer interface tools.</p>

                <h4>Lack of Capability or Robustness</h4>
                <p>This happens when the tool/AI system fails to perform reliably or effectively under varying conditions, exposing them to errors and failures that can have significant consequences, especially in critical applications or areas that require moral reasoning (healthcare, legal sector, etc.). This is also probably one of the most common risks of all tools, which is why the 3-Phase approach is so critical.</p>

                <h4>Lack of Transparency or Interpretability</h4>
                <p><em>For IRBs:</em> Without transparency, participants can't truly give informed consent. When there are challenges in understanding or explaining the decision-making processes of AI systems, this can lead to mistrust, difficulty in enforcing compliance standards or holding relevant actors accountable for harms, and the inability to identify and correct errors. One simple example (there are more complicated ones) is prompts for LLMs. When LLMs can be easily manipulated and it is not transparent what is being prompted, there can be manipulated output that fits a certain agenda while not meeting another, such as legal requirements.</p>

                <h4>AI Welfare and Rights</h4>
                <p>This is probably more rare and not a common consideration as of today (2025). However, there are many who highlight ethical considerations regarding the treatment of potentially sentient AI entities (there is also numerous literature arguing against "sentient AI"). However, the concerns discuss AI systems' potential "rights and welfare", if/when AI systems become more advanced and autonomous.</p>

                <h4>Multi-Agent Risks</h4>
                <p><em>For IRBs:</em> Multi-agent AI interactions could create new systemic risks IRBs must account for. A bit more nuanced. An "agent" is the software or "robot" that can act on its own to achieve a goal. But when we have a multi-agent system, that means we have many agents that can either be working together, interacting but doing different things, or working across different organizations in a shared space. When these multi-agents work together, we see risks of conflicting goals, collusion where agents team up in ways that harm fairness; chain reaction (that cascading failure example where one small problem ripples into the whole system); selection pressure where certain agents attempt to survive and become more aggressive or less "ethical". There are security holes where one AI may try to trick the other into revealing private data, for example; and lack of trust and missing information (when agents don't share accurate information).</p>

                <h3>Governance & Mitigation Frameworks</h3>

                <h4>Board Structure & Oversight</h4>
                <p><em>For IRBs:</em> Independent oversight is required for human subjects protections. Governance structures and leadership roles that establish executive accountability for AI safety and risk management. Examples: Dedicated risk committees, safety teams, ethics boards, crisis simulation training, multi-party authorization protocols, deployment veto powers.</p>

                <h4>Risk Management</h4>
                <p><em>For IRBs:</em> Pre-deployment risk assessment parallels investigational plan requirements. Systematic methods that identify, evaluate, and manage AI risks for comprehensive risk governance across organizations. Examples: Enterprise risk management frameworks, risk registers with capability thresholds, compliance programs, pre-deployment risk assessments, independent risk assessments.</p>

                <h4>Conflict of Interest Protections</h4>
                <p><em>For IRBs:</em> This applies to avoiding bias in research conduct. Governance mechanisms that manage financial interests and organizational structures to ensure leadership can prioritize safety over profit motives in critical situations. Examples: Background checks for key personnel, windfall profit redistribution plans, stake limitation policies, protections against shareholder pressure.</p>

                <h4>Whistleblower Reporting & Protection</h4>
                <p>Policies and systems that enable confidential reporting of safety concerns or ethical violations to prevent retaliation and encourage disclosure of risks. Examples: Anonymous reporting channels, non-retaliation guarantees, limitations on non-disparagement agreements, external whistleblower handling services.</p>

                <h4>Safety Decision Frameworks</h4>
                <p>Protocols and commitments that constrain decision-making about model development, deployment, and capability scaling, and govern safety-capability resource allocation to prevent unsafe AI advancement. Examples: If-then safety protocols, capability ceilings, deployment pause triggers, safety-capability resource ratios.</p>

                <h4>Environmental Impact Management</h4>
                <p>Processes for measuring, reporting, and reducing the environmental footprint of AI systems to ensure sustainability and responsible resource use. Examples: Carbon footprint assessment, emission offset programs, energy efficiency optimization, resource consumption tracking.</p>

                <h4>Societal Impact Assessment</h4>
                <p><em>For IRBs:</em> Processes that assess AI systems' effects on society, including impacts on employment, power dynamics, political processes, and cultural values. Examples: Fundamental rights impact assessments, expert consultations on risk domains, stakeholder engagement processes, governance gap analysis.</p>

                <h4>Testing & Auditing</h4>
                <p><em>For IRBs:</em> Verifying AI performance is equivalent to device validation. Systematic internal and external evaluations that assess AI systems, infrastructure, and compliance processes to identify risks, verify safety, and ensure performance meets standards. Examples: Third-party audits, red teaming, penetration testing, dangerous capability evaluations, bug bounty programs.</p>

                <h4>Data Governance</h4>
                <p><em>For IRBs:</em> Secure handling of data per IRB oversight. Policies and procedures that govern responsible data acquisition, curation, and usage to ensure compliance, quality, user privacy, and removal of harmful content. Examples: Harmful content filtering protocols, compliance checks for data collection standards, user data privacy controls, data curation processes.</p>

                <h4>Access Management</h4>
                <p>Operational policies and verification systems that govern who can use AI systems and for what purposes to prevent safety circumvention, deliberate misuse, and deployment in high-risk contexts. Examples: KYC verification requirements, API-only access controls, fine-tuning restrictions, acceptable use policies, high-stakes application prohibitions.</p>

                <h4>Staged Deployment</h4>
                <p>Implementation protocols that deploy AI systems in stages, requiring safety validation before expanding user access or capabilities. Examples: Limited API access programs, gradual user base expansion, capability threshold assessments, pre-deployment validation checkpoints, treating model updates as new deployments.</p>

                <h4>Post-Deployment Monitoring</h4>
                <p><em>For IRBs:</em> Ongoing safety surveillance should match post-market monitoring. Ongoing monitoring processes that track AI behavior, user interactions, and societal impacts post-deployment to detect misuse, emergent dangerous capabilities, and harmful effects. Examples: User interaction tracking systems, capability evolution assessments, periodic impact reports, automated misuse detection, usage pattern analysis tools.</p>

                <h4>Incident Response & Recovery</h4>
                <p>Protocols and technical systems that respond to security incidents, safety failures, or capability misuse to contain harm and restore safe operations. Examples: Incident response plans, emergency shutdown/rollback procedures, model containment mechanisms, safety drills, critical infrastructure protection measures.</p>

                <h4>Model & Infrastructure Security</h4>
                <p><em>For IRBs subject to 21 CFR:</em> Device software must be secure from tampering. Technical and physical safeguards that secure AI models, weights, and infrastructure to prevent unauthorized access, theft, tampering, and espionage. Examples: Model weight tracking systems, multi-factor authentication protocols, physical access controls, background security checks, compliance with information security standards.</p>

                <h4>Model Alignment</h4>
                <p>Technical methods to ensure AI systems understand and adhere to human values and intentions. Examples: Reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), constitutional AI training, value alignment verification systems.</p>

                <h4>Model Safety Engineering</h4>
                <p>Technical methods and safeguards that constrain model behaviors and protect against exploitation and vulnerabilities. Examples: Safety analysis protocols, capability restriction mechanisms, hazardous knowledge unlearning techniques, input/output filtering systems, defense-in-depth implementations, adversarial robustness training, hierarchical auditing, action replacement.</p>

                <h4>Content Safety Controls</h4>
                <p><em>For IRBs:</em> Safeguards prevent harm from toxic/triggering content (generative AI tools). Technical systems and processes that detect, filter, and label AI-generated content to identify misuse and enable content provenance tracking. Examples: Synthetic media watermarking, content filtering mechanisms, prohibited content detection, metadata tagging protocols, deepfake creation restrictions.</p>

                <h4>System Documentation</h4>
                <p><em>For IRBs:</em> Detailed documentation is a regulatory requirement. Comprehensive documentation protocols that record technical specifications, intended uses, capabilities, and limitations of AI systems to enable informed evaluation of governance. Examples: Model cards, system architecture documentation, compute resource disclosures, safety test result reports, system prompt, model specifications.</p>

                <h4>Risk Disclosure</h4>
                <p><em>For IRBs:</em> All known risks must be disclosed to participants. Formal reporting protocols and notification systems that communicate risk information, mitigation plans, safety evaluations, and significant AI activities to enable external oversight and inform stakeholders. Examples: Publishing risk assessment summaries, pre-deployment notifications to government, reporting large training runs, disclosing mitigation strategies, notifying affected parties.</p>

                <h4>Incident Reporting</h4>
                <p>Formal processes and protocols that document and share AI safety incidents, security breaches, near-misses, and relevant threat intelligence with appropriate stakeholders to enable coordinated responses and systemic improvements. Examples: Cyber threat intelligence sharing networks, mandatory breach notification procedures, incident database contributions, cross-industry safety reporting mechanisms, standardized near-miss documentation protocols.</p>

                <h4>Governance Disclosure</h4>
                <p>Formal disclosure mechanisms that communicate governance structures, decision frameworks, and safety commitments to enhance transparency and enable external oversight of high-stakes AI decisions. Examples: Published safety and/or alignment strategies, governance documentation safety cases, model registration protocols, public commitment disclosures.</p>

                <h4>Third-Party System Access</h4>
                <p>Mechanisms granting controlled system access to vetted external parties to enable independent assessment, validation, and safety research of AI models and capabilities. Examples: Research access programs, third-party capability assessments, government access provisions, legal safe harbors for public interest evaluations.</p>

                <h4>User Rights & Recourse</h4>
                <p>Framework and procedures that enable users to identify and understand AI system interactions, report issues, request explanations, and seek recourse or remediation when affected by AI systems. Examples: User reporting channels, appeal processes, explanation request systems, remediation protocols, content verification.</p>

                <h3>Key Regulatory Frameworks</h3>
                <ul>
                    <li><strong>45 CFR 46 (Common Rule):</strong> Federal policy for protection of human research subjects</li>
                    <li><strong>21 CFR 56:</strong> FDA regulations for IRB oversight</li>
                    <li><strong>21 CFR 812:</strong> Investigational Device Exemptions (IDE)</li>
                    <li><strong>HIPAA Privacy Rule:</strong> Standards for protecting health information</li>
                    <li><strong>ISO 14971:</strong> International standard for risk management in medical devices</li>
                </ul>

                <h3>Belmont Principles Applied to AI</h3>
                <ul>
                    <li><strong>Respect for Persons:</strong> Informed consent about AI use, explainability requirements</li>
                    <li><strong>Beneficence:</strong> Maximizing benefits and minimizing harms from AI systems</li>
                    <li><strong>Justice:</strong> Fair distribution of AI benefits and burdens across populations</li>
                </ul>
            </div>
        </section>

        <!-- About Section -->
        <section id="about" class="section">
            <div class="card">
                <h2>About This Tool</h2>
                
                <h3>Development & Validation</h3>
                <p>The AI HSR Risk Reference Tool was developed through a structured, iterative design process as part of a safety engineering project at the Center for AI Safety (CAIS). The tool has been:</p>
                <ul>
                    <li>Validated at 23+ institutions nationally</li>
                    <li>Shown to improve reviewer confidence by 21%</li>
                    <li>Demonstrated 60% reduction in revision cycles</li>
                    <li>Adopted by institutions including University of Washington and Multi-Regional Clinical Trials Center</li>
                </ul>

                <h3>Methodology</h3>
                <p>The tool maps risks and safeguards from the MIT AI Risk Library and MIT AI Risk Mitigation Library against:</p>
                <ul>
                    <li>ISO 14971 (risk management for medical devices)</li>
                    <li>45 CFR 46 (Common Rule)</li>
                    <li>21 CFR Parts 312, 812, and 820 (FDA regulations)</li>
                    <li>HIPAA Privacy Rule</li>
                    <li>Belmont Principles and Good Clinical Practice (GCP)</li>
                </ul>

                <h3>Scope & Limitations</h3>
                <p><strong>Current Version Includes:</strong></p>
                <ul>
                    <li>AI-specific risks under U.S. human subjects regulations (45 CFR 46)</li>
                    <li>Focus on complex AI systems (predictive models, LLMs, foundation models)</li>
                    <li>Four core risk domains relevant to HSR</li>
                </ul>

                <p><strong>Future Versions Will Include:</strong></p>
                <ul>
                    <li>International regulations (EU AI Act, GDPR)</li>
                    <li>ISO standards (42001, 23894, 42005, 24368)</li>
                    <li>Patient and community perspectives</li>
                    <li>Integration with IRB electronic platforms</li>
                </ul>

                <h3>How to Use This Tool</h3>
                <div class="info-box">
                    <ol>
                        <li>Navigate to the <strong>Interactive Tool</strong> section</li>
                        <li>Select the development phase of the AI system under review</li>
                        <li>Choose relevant risk domains</li>
                        <li>Review identified risks, mitigation strategies, and reviewer prompts</li>
                        <li>Use prompts to guide IRB deliberations</li>
                        <li>Document findings in your IRB review materials</li>
                    </ol>
                </div>

                <h3>Citation</h3>
                <p>If you use this tool in your work, please cite:</p>
                <p style="font-style: italic; padding-left: 2rem;">
                    Eto, T. (2025). AI HSR Risk Reference Tool v2.0: Quick Reference Risk Identification and Mitigation Guide for IRBs Reviewing AI in Human Subjects Research. TechInHSR.
                </p>

                <h3>Resources</h3>
                <ul>
                    <li><a href="https://techinhsr.com" target="_blank">TechInHSR.com</a> - Blog and updates</li>
                    <li><a href="https://github.com/etotamikolab/TechInHSR" target="_blank">GitHub Repository</a> - Access the Excel version</li>
                    <li><a href="https://ai-hsr-risk-reference.glide.page" target="_blank">Glide App v1.5</a> - Previous web version</li>
                </ul>

                <h3>Acknowledgements</h3>
                <p>Special thanks to Professor Josep Curto, PhD, at the Center for AI Safety (CAIS) for invaluable guidance, and to colleagues Mark Lifson, Heather Miller, and the broader IRB community for their feedback and support.</p>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Tamiko Eto, MA CIP | TechInHSR</p>
            <p style="margin-top: 0.5rem;">
                <strong>Licensed under:</strong> 
                <a href="https://www.gnu.org/licenses/agpl-3.0.en.html" target="_blank">AGPL-3.0</a> (code) & 
                <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> (content)
                <br>
                <span style="font-size: 0.875rem; opacity: 0.9;">Free for non-commercial use | Commercial licensing: <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="e195808c888a8ea195848289888f899293cf828e8c">[email&#160;protected]</a></span>
            </p>
            <p style="margin-top: 0.5rem;">
                <a href="/cdn-cgi/l/email-protection#5226333f3b393d122637313a3b3c3a21207c313d3f">Contact</a> | 
                <a href="https://techinhsr.com" target="_blank">Website</a> | 
                <a href="https://github.com/[your-username]/ai-hsr-risk-tool" target="_blank">GitHub</a>
            </p>
        </div>
    </footer>

    <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>
        // Risk data structure
        const risks = [
            {
                id: 1,
                title: "Algorithmic Bias in Training Data",
                phases: ["phase1", "phase2", "phase3"],
                domains: ["discrimination"],
                specificRisk: "equity",
                description: "AI models trained on biased or non-representative datasets may perpetuate or amplify existing disparities, leading to unfair treatment of certain demographic groups.",
                mitigations: [
                    "Conduct comprehensive demographic analysis of training data",
                    "Implement bias detection algorithms during model development",
                    "Use diverse, representative datasets that reflect the target population",
                    "Document data sources and known limitations",
                    "Perform fairness audits across protected characteristics"
                ],
                reviewerPrompts: {
                    phase1: "Has the investigator documented the demographic composition of training data? Are there known gaps in representation?",
                    phase2: "What fairness metrics were used to evaluate model performance across subgroups? Are disparities documented?",
                    phase3: "How will ongoing monitoring detect emerging bias in real-world deployment? What is the plan for addressing identified disparities?"
                }
            },
            {
                id: 2,
                title: "Model Opacity and Lack of Explainability",
                phases: ["phase2", "phase3"],
                domains: ["hci"],
                specificRisk: "explainability",
                description: "Complex AI models (deep learning, ensemble methods) may function as 'black boxes' where the reasoning behind predictions is unclear to researchers, clinicians, and participants.",
                mitigations: [
                    "Implement explainable AI (XAI) techniques (SHAP, LIME, attention mechanisms)",
                    "Provide feature importance rankings for predictions",
                    "Create user-friendly visualizations of model reasoning",
                    "Document model limitations in plain language",
                    "Establish thresholds for when human expert review is required"
                ],
                reviewerPrompts: {
                    phase2: "Can the investigator explain how the model arrives at predictions? What explainability methods are being used?",
                    phase3: "How will clinicians and participants understand AI recommendations? Is there a process for explaining predictions in individual cases?"
                }
            },
            {
                id: 3,
                title: "Misclassification and Clinical Error",
                phases: ["phase2", "phase3"],
                domains: ["misinformation"],
                specificRisk: "misclassification",
                description: "AI systems may incorrectly categorize diagnoses, risk levels, or treatment recommendations, potentially leading to inappropriate interventions or missed necessary care.",
                mitigations: [
                    "Establish clear performance thresholds (sensitivity, specificity, PPV, NPV)",
                    "Implement confidence scoring for predictions",
                    "Require human verification for high-stakes decisions",
                    "Create error detection and reporting mechanisms",
                    "Define clear protocols for handling uncertain predictions",
                    "Monitor false positive and false negative rates by subgroup"
                ],
                reviewerPrompts: {
                    phase2: "What are the model's error rates? How do they compare to human performance or current standards?",
                    phase3: "What safeguards prevent harmful actions based on misclassifications? How are errors caught and corrected?"
                }
            },
            {
                id: 4,
                title: "Data Privacy and Re-identification Risk",
                phases: ["phase1", "phase2", "phase3"],
                domains: ["privacy"],
                specificRisk: "data-privacy",
                description: "Large AI models may enable re-identification of participants through data linkage, model inversion attacks, or membership inference, compromising confidentiality.",
                mitigations: [
                    "Apply appropriate de-identification techniques (HIPAA Safe Harbor, Expert Determination)",
                    "Implement differential privacy in model training",
                    "Conduct Privacy Impact Assessments (PIAs)",
                    "Restrict data access through secure computing environments",
                    "Use federated learning to keep data distributed",
                    "Implement query restrictions to prevent inference attacks",
                    "Establish data sharing agreements with clear limitations"
                ],
                reviewerPrompts: {
                    phase1: "What identifiers are present in the training data? How is de-identification verified?",
                    phase2: "Could the model reveal information about training data individuals? What privacy-preserving techniques are used?",
                    phase3: "What controls prevent unauthorized access to participant data? How is HIPAA compliance maintained?"
                }
            },
            {
                id: 5,
                title: "Inadequate Informed Consent for AI Use",
                phases: ["phase2", "phase3"],
                domains: ["hci"],
                specificRisk: "explainability",
                description: "Participants may not adequately understand how AI will be used in their care or research participation, limiting truly informed consent.",
                mitigations: [
                    "Use plain language to explain AI involvement",
                    "Describe what data will be used and how",
                    "Explain limitations and potential errors",
                    "Clarify human oversight and decision-making authority",
                    "Provide examples of AI predictions/recommendations",
                    "Allow opt-out options where appropriate",
                    "Address future use and data retention"
                ],
                reviewerPrompts: {
                    phase2: "Does the consent form adequately explain AI use in terms participants can understand?",
                    phase3: "Are participants informed about the role of AI in clinical decisions? Can they opt out of AI-based recommendations?"
                }
            },
            {
                id: 6,
                title: "Automation Bias and Over-Reliance",
                phases: ["phase3"],
                domains: ["hci"],
                specificRisk: "misclassification",
                description: "Clinicians and researchers may over-rely on AI recommendations, failing to apply independent judgment, especially when the AI appears confident.",
                mitigations: [
                    "Training programs emphasizing AI as decision support, not replacement",
                    "Interface design that encourages critical thinking",
                    "Display confidence intervals and uncertainty metrics",
                    "Require documentation of clinical reasoning independent of AI",
                    "Monitor decision patterns for automation bias",
                    "Establish protocols for overriding AI recommendations"
                ],
                reviewerPrompts: {
                    phase3: "What training will clinicians receive on appropriate AI use? How is independent judgment preserved?"
                }
            },
            {
                id: 7,
                title: "Disparate Performance Across Subgroups",
                phases: ["phase2", "phase3"],
                domains: ["discrimination"],
                specificRisk: "equity",
                description: "AI models may perform significantly better for some demographic groups than others, exacerbating health disparities.",
                mitigations: [
                    "Conduct stratified performance analysis by race, ethnicity, sex, age, etc.",
                    "Establish minimum performance thresholds for all subgroups",
                    "Use fairness-aware machine learning techniques",
                    "Oversample underrepresented groups in training data",
                    "Implement continuous monitoring of subgroup performance",
                    "Create protocols for addressing identified disparities"
                ],
                reviewerPrompts: {
                    phase2: "Has the model been validated across demographic subgroups? What are the performance differences?",
                    phase3: "How will disparate performance be monitored and addressed in real-world use?"
                }
            },
            {
                id: 8,
                title: "AI Hallucinations and Fabricated Information",
                phases: ["phase2", "phase3"],
                domains: ["misinformation"],
                specificRisk: "misclassification",
                description: "Generative AI systems (especially LLMs) may produce plausible but false information, including fabricated references, diagnoses, or recommendations.",
                mitigations: [
                    "Implement fact-checking and verification systems",
                    "Ground outputs in verified data sources",
                    "Display confidence scores and sources",
                    "Require human verification of critical information",
                    "Create feedback mechanisms for identifying errors",
                    "Use retrieval-augmented generation (RAG) architectures",
                    "Establish clear disclaimers about AI limitations"
                ],
                reviewerPrompts: {
                    phase2: "How are AI hallucinations detected and prevented? What verification processes are in place?",
                    phase3: "What safeguards prevent clinicians from acting on fabricated information?"
                }
            },
            {
                id: 9,
                title: "Inadequate Model Validation",
                phases: ["phase2"],
                domains: ["misinformation"],
                specificRisk: "misclassification",
                description: "AI models deployed without rigorous validation may perform poorly in real-world settings, leading to harmful errors.",
                mitigations: [
                    "Conduct internal and external validation studies",
                    "Test on diverse, representative populations",
                    "Compare performance to established standards or human experts",
                    "Assess calibration (predicted vs. actual risk)",
                    "Evaluate performance across relevant clinical scenarios",
                    "Document validation results transparently"
                ],
                reviewerPrompts: {
                    phase2: "What validation studies have been conducted? Were they internal or external? How does performance compare to benchmarks?"
                }
            },
            {
                id: 10,
                title: "Data Security and Breach Risk",
                phases: ["phase1", "phase2", "phase3"],
                domains: ["privacy"],
                specificRisk: "data-privacy",
                description: "Large datasets required for AI development create expanded attack surfaces for data breaches, unauthorized access, or cyberattacks.",
                mitigations: [
                    "Implement robust encryption (at rest and in transit)",
                    "Use multi-factor authentication and access controls",
                    "Conduct regular security audits and penetration testing",
                    "Establish incident response plans",
                    "Limit data access to minimum necessary",
                    "Use secure computing environments (e.g., trusted research environments)",
                    "Comply with relevant security standards (NIST, HITRUST)"
                ],
                reviewerPrompts: {
                    phase1: "What security measures protect the training data? Who has access?",
                    phase2: "How is the model secured against unauthorized access or tampering?",
                    phase3: "What is the plan for responding to a data breach?"
                }
            },
            {
                id: 11,
                title: "Secondary Use and Data Drift",
                phases: ["phase1", "phase3"],
                domains: ["privacy"],
                specificRisk: "data-privacy",
                description: "Data collected for one purpose may be used to train future AI models, and model performance may degrade as real-world data distributions change over time.",
                mitigations: [
                    "Explicitly address secondary use in consent forms",
                    "Establish data governance policies for future AI development",
                    "Implement continuous monitoring for data drift",
                    "Re-validate models periodically",
                    "Create triggers for model retraining or retirement",
                    "Document all uses of participant data"
                ],
                reviewerPrompts: {
                    phase1: "Does the consent address potential future uses of data for AI development?",
                    phase3: "How will model performance be monitored over time? When will revalidation occur?"
                }
            },
            {
                id: 12,
                title: "Vulnerable Population Considerations",
                phases: ["phase1", "phase2", "phase3"],
                domains: ["discrimination"],
                specificRisk: "equity",
                description: "AI systems may pose heightened risks to vulnerable populations (children, prisoners, cognitive impairment) who may not fully understand AI involvement or whose data requires additional protection.",
                mitigations: [
                    "Conduct additional safeguards for vulnerable populations",
                    "Simplify consent processes and explanations",
                    "Involve legally authorized representatives appropriately",
                    "Assess capacity for understanding AI involvement",
                    "Implement enhanced privacy protections",
                    "Consider whether AI benefits justify risks for vulnerable groups"
                ],
                reviewerPrompts: {
                    phase1: "Does the study involve vulnerable populations? Are additional protections planned?",
                    phase2: "How is consent modified for participants with limited understanding?",
                    phase3: "What safeguards address heightened risks to vulnerable participants?"
                }
            }
        ];

        // Navigation function
        function showSection(sectionId) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Remove active class from all buttons
            document.querySelectorAll('.nav-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(sectionId).classList.add('active');
            
            // Add active class to clicked button
            event.target.classList.add('active');

            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Filter risks based on selections
        function filterRisks() {
            const phase = document.getElementById('phaseFilter').value;
            const domain = document.getElementById('domainFilter').value;
            const risk = document.getElementById('riskFilter').value;

            const filtered = risks.filter(r => {
                const phaseMatch = phase === 'all' || r.phases.includes(phase);
                const domainMatch = domain === 'all' || r.domains.includes(domain);
                const riskMatch = risk === 'all' || r.specificRisk === risk;
                return phaseMatch && domainMatch && riskMatch;
            });

            displayRisks(filtered);
        }

        // Display filtered risks
        function displayRisks(risksToDisplay) {
            const container = document.getElementById('risksContainer');
            
            if (risksToDisplay.length === 0) {
                container.innerHTML = '<div class="no-results"><h3>No risks match your filters</h3><p>Try adjusting your selection criteria</p></div>';
                return;
            }

            container.innerHTML = risksToDisplay.map(risk => `
                <div class="risk-card">
                    <div class="risk-title">${risk.title}</div>
                    <div class="risk-meta">
                        ${risk.phases.map(p => {
                            const phaseNames = {
                                phase1: 'Phase 1',
                                phase2: 'Phase 2',
                                phase3: 'Phase 3'
                            };
                            return `<span class="badge badge-${p}">${phaseNames[p]}</span>`;
                        }).join('')}
                        ${risk.domains.map(d => {
                            const domainNames = {
                                discrimination: 'Discrimination',
                                privacy: 'Privacy',
                                misinformation: 'Misinformation',
                                hci: 'HCI'
                            };
                            return `<span class="badge badge-domain">${domainNames[d]}</span>`;
                        }).join('')}
                    </div>
                    <div class="risk-description">${risk.description}</div>
                    
                    <div class="mitigation-section">
                        <div class="section-title">üõ°Ô∏è Mitigation Strategies</div>
                        <ul>
                            ${risk.mitigations.map(m => `<li>${m}</li>`).join('')}
                        </ul>
                    </div>

                    <div class="reviewer-section">
                        <div class="section-title">‚úì Reviewer Prompts</div>
                        ${Object.entries(risk.reviewerPrompts).map(([phase, prompt]) => {
                            const phaseNames = {
                                phase1: 'Phase 1',
                                phase2: 'Phase 2',
                                phase3: 'Phase 3'
                            };
                            return `<p><strong>${phaseNames[phase]}:</strong> ${prompt}</p>`;
                        }).join('')}
